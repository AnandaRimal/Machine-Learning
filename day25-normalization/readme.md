# Normalization (Min-Max Scaling)

## 1. Introduction
While Standardization centers data around zero, **Normalization** (specifically Min-Max Scaling) squeezes data into a fixed box, typically between **0 and 1**. This is particularly useful when you need your data to be bounded or when the underlying distribution is not Gaussian.

![Illustration of data being squashed into a 0-1 range generated by Nano Banana Model](https://via.placeholder.com/800x400?text=Normalization+Process+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### The Squeeze
Normalization transforms features by scaling the data to a fixed range. The formula is:

$$x_{scaled} = \frac{x - x_{min}}{x_{max} - x_{min}}$$

- If $x = x_{min}$, the result is 0.
- If $x = x_{max}$, the result is 1.
- Everything else falls proportionally in between.

### When to use Normalization vs. Standardization?
- **Use Normalization** when:
    - You know the distribution is **not Gaussian** (e.g., uniform distribution).
    - You are using algorithms that require bounded inputs (e.g., Neural Networks often prefer 0-1 inputs).
    - You are working with Image Data (pixel intensities are 0-255, so scaling to 0-1 is natural).
- **Use Standardization** otherwise (it's generally more robust to outliers).

## 3. Visualizing the Concept
*(Imagine a diagram here generated by the Nano Banana Model showing a "Data Compactor" machine pressing data points into a 0-1 box)*

## 4. Practical Implementation & Notebook Walkthrough

The notebook `day25.ipynb` uses the `wine_data.csv` dataset.

### 4.1 Setup and Split
As always, we split the data first to avoid leakage.
```python
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
```

### 4.2 Applying MinMaxScaler
We use Scikit-Learn's `MinMaxScaler`.
```python
from sklearn.preprocessing import MinMaxScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
scaler = MinMaxScaler()

# fit() learns the min and max of the Train data
scaler.fit(X_train)

# transform() applies the scaling
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

Pipelines keep scaling tied to the model:
```python
numeric = X.columns.tolist()
pre = ColumnTransformer([
    ('num', MinMaxScaler(), numeric)
], remainder='drop')

clf = Pipeline([
    ('pre', pre),
    ('model', LogisticRegression(max_iter=1000))
])
clf.fit(X_train, y_train)
print(clf.score(X_test, y_test))
```

### 4.3 The Outlier Problem
The notebook demonstrates a critical weakness of Normalization: **Outliers**.
If you have a salary column with values [10k, 20k, 30k, 10M], the 10M outlier becomes 1.0, and the rest of the data gets squashed into a tiny range like [0.001, 0.003]. This destroys the variance in the majority of your data.
*Insight*: Always check for and handle outliers before using Min-Max Scaling.

```python
# Winsorize to mitigate extreme outliers before scaling
import numpy as np
q_lo, q_hi = X_train.quantile([0.01, 0.99])
X_train_clipped = X_train.clip(lower=q_lo, upper=q_hi, axis=1)
X_test_clipped = X_test.clip(lower=q_lo, upper=q_hi, axis=1)
```

### 4.4 Visualization
We plot the data before and after.
- **Before**: Features have different ranges (Alcohol vs Malic Acid).
- **After**: Both features are strictly within [0, 1]. The geometry of the scatterplot is preserved, but the axes are normalized.

## 5. Summary
Normalization is a precise tool. It's perfect for image processing and specific neural network architectures, but it requires clean data free of extreme outliers to work effectively.
