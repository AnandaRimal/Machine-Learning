# Ordinal Encoding & Label Encoding

## 1. Introduction
Machine Learning models are mathematical enginesâ€”they understand numbers, not text. You cannot feed "Red", "Green", "Blue" or "Low", "Medium", "High" directly into a regression or classification model. You must convert these categories into numbers. This chapter explores two fundamental techniques for this: **Ordinal Encoding** and **Label Encoding**.

![Illustration showing text categories being converted to numbers 0, 1, 2 generated by Nano Banana Model](https://via.placeholder.com/800x400?text=Categorical+Encoding+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### The Hierarchy of Categories
Not all categorical data is created equal.
1.  **Nominal Data**: Categories with no inherent order (e.g., City: New York, Paris, Tokyo).
2.  **Ordinal Data**: Categories with a clear rank or order (e.g., Size: Small, Medium, Large).

### Ordinal Encoding
Used for **Ordinal Data**. It assigns integers based on the rank.
- Small -> 0
- Medium -> 1
- Large -> 2
*Crucial*: The model interprets 2 > 1 > 0, which captures the "size" relationship.

**Math (Monotonicity)**
- For ordinal feature $x \in \{c_1, c_2, ..., c_k\}$ with order $c_1 < c_2 < ... < c_k$, mapping $\phi(c_i)=i$ preserves monotonic relationships in linear models: if coefficient $w>0$, predicted outcome increases with category rank.

### Label Encoding
Used specifically for the **Target Variable (y)**. It assigns unique integers to classes.
- Cat -> 0
- Dog -> 1
*Warning*: If used on input features (X) that are nominal, the model might incorrectly learn that Dog > Cat, which is mathematically nonsensical.

**Pros/Cons**
- Pros: Compact representation; preserves order; works well for tree models too.
- Cons: Misuse on nominal features induces spurious ordering; sensitive if order changes.

## 3. Visualizing the Concept
*(Imagine a flowchart here generated by the Nano Banana Model: Is it a Feature or Target? -> Feature -> Is it Ordered? -> Yes (Ordinal) / No (One-Hot) -> Target -> Label Encoding)*

## 4. Practical Implementation & Notebook Walkthrough

The notebook `day26.ipynb` demonstrates both techniques using a customer dataset.

### 4.1 Ordinal Encoding (for Features)
We use `OrdinalEncoder` from `sklearn.preprocessing`.
```python
from sklearn.preprocessing import OrdinalEncoder

# Define the specific order for each column
categories = [
    ['Poor', 'Average', 'Good'],  # Review
    ['School', 'UG', 'PG']        # Education
]

oe = OrdinalEncoder(categories=categories)
X_train_encoded = oe.fit_transform(X_train)
```
*Note*: We explicitly pass the `categories` list so the encoder knows that 'Good' > 'Average' > 'Poor'.

```python
# Integrate with pipeline
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

ord_cols = ['Review','Education']
pre = ColumnTransformer([
    ('ord', OrdinalEncoder(categories=categories), ord_cols)
], remainder='drop')

clf = Pipeline([
    ('pre', pre),
    ('model', LogisticRegression(max_iter=1000))
])
clf.fit(X_train, y_train)
```

### 4.2 Label Encoding (for Target)
We use `LabelEncoder` from `sklearn.preprocessing`.
```python
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train)
```
This is strictly for the output variable (e.g., 'Purchased': Yes/No).

## 5. Summary
- Use **Ordinal Encoding** for input features (X) that have an order.
- Use **Label Encoding** for the target variable (y).
- Never use Label Encoding for nominal input features (use One-Hot Encoding instead).
 - Prefer pipelines to bind encoders to downstream models and avoid leakage.
