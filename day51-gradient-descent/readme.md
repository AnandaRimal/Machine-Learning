# Gradient Descent

## 1. Introduction
The Normal Equation (used in Day 50) is great, but it's slow if you have millions of rows ($O(n^3)$ complexity).
**Gradient Descent** is an optimization algorithm that finds the best coefficients iteratively. It's like walking down a mountain blindfolded: you feel the slope under your feet and take a step downhill.

![Illustration of a hiker walking down a 3D valley towards the lowest point generated by Nano Banana Model](https://via.placeholder.com/800x400?text=Gradient+Descent+Hiker+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### The Cost Function (Loss)
We define a "Cost Function" (usually MSE) that measures how bad our model is. Our goal is to minimize this cost.
$$ J(m, b) = \frac{1}{n} \sum (y - (mx + b))^2 $$
This function creates a bowl shape (convex). The bottom of the bowl is the best model.

### The Algorithm
1.  Start with random $m$ and $b$.
2.  Calculate the **Gradient** (slope) of the cost function at that point.
3.  Move in the opposite direction of the gradient.
4.  Repeat until you reach the bottom (convergence).

### Learning Rate ($\alpha$)
The size of the step you take.
- **Too small**: Takes forever to reach the bottom.
- **Too big**: You might overshoot the bottom and diverge.

## 3. Visualizing the Concept
*(Imagine a ball rolling down a bowl. It starts fast, then slows down as the slope gets flatter, finally settling at the center generated by the Nano Banana Model)*

## 4. Practical Implementation & Notebook Walkthrough

The notebook `gradient-descent-code-from-scratch.ipynb` implements this manually.

### 4.1 The Update Rule
We derive the partial derivatives with respect to $m$ and $b$:
$$ m_{new} = m_{old} - \alpha \times \frac{\partial J}{\partial m} $$
$$ b_{new} = b_{old} - \alpha \times \frac{\partial J}{\partial b} $$

### 4.2 Python Implementation
```python
for i in range(epochs):
    loss_slope_b = -2 * np.sum(y - m*X - b)
    loss_slope_m = -2 * np.sum((y - m*X - b)*X)
    
    b = b - (learning_rate * loss_slope_b)
    m = m - (learning_rate * loss_slope_m)
```

### 4.3 Animation
The folder contains several GIFs (`animation.gif`) showing the regression line slowly rotating and shifting until it fits the data perfectly. This is Gradient Descent in action!

## 5. Summary
Gradient Descent is the engine behind almost all modern Machine Learning and Deep Learning. Understanding how it navigates the "Loss Landscape" is crucial for tuning Neural Networks.
