# ElasticNet Regression

## 1. Introduction
Ridge is good. Lasso is good. Why not both?
**ElasticNet** is the hybrid child of Ridge and Lasso. It combines the L1 and L2 penalties. It's the safest bet when you don't know which regularization to use.

![Illustration of a rubber band (Elastic) stretching between the properties of Ridge and Lasso generated by Nano Banana Model](https://via.placeholder.com/800x400?text=ElasticNet+Hybrid+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### The Cost Function
$$ J = MSE + r \lambda \sum |\beta| + (1-r) \lambda \sum \beta^2 $$
- **$r$ (l1_ratio)**: The mix ratio.
    - $r=1$: Pure Lasso.
    - $r=0$: Pure Ridge.
    - $r=0.5$: 50/50 Mix.

### Why use it?
Lasso has a flaw: if two features are highly correlated, it picks one randomly and drops the other. ElasticNet (thanks to the Ridge part) tends to keep both or shrink them together. It's more stable than Lasso.

## 3. Visualizing the Concept
*(Imagine a control panel with two knobs: one for L1 penalty and one for L2 penalty, allowing you to fine-tune the model behavior generated by the Nano Banana Model)*

## 4. Practical Implementation & Notebook Walkthrough

The notebook `elastic-net-regression.ipynb` compares all methods on the Diabetes dataset.

### 4.1 Comparison
```python
from sklearn.linear_model import ElasticNet

# l1_ratio=0.5 means 50% Lasso, 50% Ridge
reg = ElasticNet(alpha=0.005, l1_ratio=0.5)
reg.fit(X_train, y_train)
```

### 4.2 Results
The notebook calculates R2 scores for all models. Often, ElasticNet performs slightly better or matches the best of Ridge/Lasso because it's more flexible.

## 5. Summary
- **Ridge**: Best for preventing overfitting when all features are useful.
- **Lasso**: Best for feature selection.
- **ElasticNet**: Best of both worlds, especially when features are correlated. 
