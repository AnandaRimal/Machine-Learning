# Scikit-Learn Pipelines

## 1. Introduction
If `ColumnTransformer` is about processing columns in parallel, **Pipelines** are about processing steps in **sequence**. A Machine Learning workflow involves a chain of steps: Imputation -> Encoding -> Scaling -> Feature Selection -> Model Training. A Pipeline bundles these steps into a single object that acts like a model.

![Diagram showing a linear chain of processing steps ending in a model generated by Nano Banana Model](https://via.placeholder.com/800x400?text=ML+Pipeline+Chain+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### The Chain of Command
A Pipeline is a list of `(name, transform)` tuples.
- The last step must be an **estimator** (e.g., Decision Tree, Linear Regression).
- All previous steps must be **transformers** (have `fit` and `transform` methods).

### Why use Pipelines?
1.  **Convenience**: You call `fit` and `predict` once on the pipeline, and it triggers the entire chain.
2.  **Safety**: It prevents **Data Leakage** by ensuring that statistics (mean, std, etc.) are learned *only* from the training data during cross-validation.
3.  **Hyperparameter Tuning**: You can grid search parameters for *all* steps at once (e.g., "What is the best imputation strategy AND the best tree depth?").

## 3. Visualizing the Concept
*(Imagine a plumbing diagram here generated by the Nano Banana Model: Water (Data) flows through various filters (Transformers) and finally reaches the tap (Model))*

## 4. Practical Implementation & Notebook Walkthrough

The notebook `titanic-using-pipeline.ipynb` builds a complete end-to-end pipeline.

### 4.1 Defining the Steps
We create individual transformers first (often using `ColumnTransformer`).
```python
# Step 1: Impute missing values
trf1 = ColumnTransformer(...)

# Step 2: One Hot Encoding
trf2 = ColumnTransformer(...)

# Step 3: Scaling
trf3 = ColumnTransformer(...)

# Step 4: Feature Selection
trf4 = SelectKBest(score_func=chi2, k=8)

# Step 5: Model
trf5 = DecisionTreeClassifier()
```

### 4.2 Creating the Pipeline
```python
from sklearn.pipeline import Pipeline

pipe = Pipeline([
    ('imputer', trf1),
    ('encoder', trf2),
    ('scaler', trf3),
    ('fs', trf4),
    ('model', trf5)
])
```

### 4.3 Training and Prediction
```python
# The pipeline handles the flow of data automatically
pipe.fit(X_train, y_train)

y_pred = pipe.predict(X_test)
```

### 4.4 Exporting the Pipeline
We can save the entire pipeline (preprocessing + model) as a pickle file. This is how models are deployed in production.
```python
import pickle
pickle.dump(pipe, open('pipe.pkl', 'wb'))
```

## 5. Summary
Pipelines are the mark of a mature Data Scientist. They turn a messy script into a portable, reproducible, and deployable software artifact.

