# Working with JSON and SQL

## 1. Introduction
In the modern data ecosystem, data rarely lives in isolation. It flows from Web APIs in **JSON** format and resides in robust **SQL** databases. This chapter explores how to bridge the gap between these storage formats and the analytical power of Pandas DataFrames.

![Illustration of JSON hierarchical structure vs SQL tabular structure generated by Nano Banana Model](https://via.placeholder.com/800x400?text=JSON+vs+SQL+Structure+by+Nano+Banana+Model)

# JSON and SQL: Mathematical Foundations of Data Transformation

![Nano Banana visualization of JSON tree to relational tables transformation](https://via.placeholder.com/800x400?text=JSON+to+SQL+Transformation+by+Nano+Banana)

**Overview**
- JSON: flexible, nested, semi-structured; SQL tables: rigid, typed, relational.
- Goal: ingest JSON (files or APIs), normalize to tabular form, store/query via SQL.

**Key Concepts**
- JSON structure: objects `{}`, arrays `[]`, primitives; nesting and optional fields.
- Normalization: flatten nested records into related tables; preserve keys/relationships.
- SQL schema: define types (`INT`, `VARCHAR`, `TIMESTAMP`), constraints (PK/FK), indexes.
- ETL: Extract (read JSON), Transform (clean, map, normalize), Load (write to DB).

**Math & Data Models**
- Cardinality: one-to-many (1:N) mapping from parent to child arrays; model via FK.
- Nullability: represent missing fields as `NULL`; define constraints to enforce integrity.
- Normal forms: reduce redundancy; split repeating groups into separate tables.

**Pros/Cons**
- Pros: JSON flexible; SQL powerful querying and integrity; scalable storage.
- Cons: Flattening complexity; schema drift; type coercion pitfalls; performance trade-offs.

**Code: JSON → DataFrame → SQL**
```python
import json
import pandas as pd
import sqlite3

# Read JSON file (list of records)
with open('train.json', 'r', encoding='utf-8') as f:
	data = [json.loads(line) if line.strip().startswith('{') else json.loads(f.read())][0]

# Normalize nested JSON
df = pd.json_normalize(data, sep='.')

# Type cleanup
df['timestamp'] = pd.to_datetime(df.get('timestamp'), errors='coerce')
for col in df.select_dtypes(include='float').columns:
	df[col] = pd.to_numeric(df[col], errors='coerce')

# Load to SQLite
conn = sqlite3.connect('day16.db')
df.to_sql('events', conn, if_exists='replace', index=False)

# Query SQL
q = pd.read_sql_query("""
SELECT strftime('%Y-%m', timestamp) AS ym, COUNT(*) AS n
FROM events
WHERE timestamp IS NOT NULL
GROUP BY ym
ORDER BY ym
""", conn)
print(q.head())
conn.close()
```

**Handling Nested Arrays**
```python
# Suppose each record has field `items` (array of objects)
records = pd.json_normalize(data)
items = pd.json_normalize(data, record_path=['items'], meta=['id'], sep='.')
# `items` table with FK `id` referencing parent
```

**API JSON to SQL (with pagination)**
```python
import requests, time
import pandas as pd
import sqlite3

def fetch_all(url, params=None, sleep=0.2):
	items = []
	page = 1
	while True:
		r = requests.get(url, params={**(params or {}), 'page': page}, timeout=20)
		r.raise_for_status()
		payload = r.json()
		batch = payload.get('results', payload)
		if not batch:
			break
		items.extend(batch)
		page += 1
		time.sleep(sleep)
	return items

rows = fetch_all('https://api.example.com/events')
df = pd.json_normalize(rows)
conn = sqlite3.connect('api.db')
df.to_sql('events', conn, if_exists='replace', index=False)
conn.close()
```

**Reliability Tips**
- Validate schema: assert expected columns; log unknown fields.
- Handle types: explicit coercion; set `errors='coerce'` and audit `NaN` rates.
- Idempotent loads: write to staging, then swap into production tables.
- Indexing: create indexes on join keys and time columns.

**Summary**
- Normalize nested JSON to related tables, enforce schema via SQL, and use pandas for transformation. Balance flexibility of JSON with rigor of SQL for reliable analytics.
## 2. Conceptual Deep Dive

### JSON (JavaScript Object Notation)
JSON is the standard format for data interchange on the web. Unlike the flat, row-column structure of CSVs, JSON is **hierarchical** and **semi-structured**.
- **Key-Value Pairs**: Data is stored as `key: value`.
- **Nesting**: Values can be lists or other objects, creating a tree-like structure.

### SQL (Structured Query Language)
SQL databases (MySQL, PostgreSQL) store data in **relational tables** with strict schemas.
- **ACID Properties**: Ensures data integrity.
- **Relationships**: Tables are linked via Foreign Keys.

### The ETL Process
The notebook in this folder performs a mini-ETL (Extract, Transform, Load) process:
1.  **Extract**: Pull data from a JSON file or SQL database.
2.  **Transform**: Convert nested JSON or SQL result sets into a flat Pandas DataFrame.
3.  **Load**: Ready for analysis.

## 3. Visualizing the Concept
*(Imagine a diagram here generated by the Nano Banana Model showing a tree structure (JSON) being flattened into a 2D matrix (DataFrame), and a SQL query pipeline feeding into a DataFrame)*

## 4. Practical Implementation & Notebook Walkthrough

The notebook `day16.ipynb` covers two distinct sections:

### 4.1 Working with JSON
We use `pd.read_json()` to handle JSON data.

**Loading Local JSON:**
```python
import pandas as pd
df = pd.read_json('train.json')
```
This reads a local file. Pandas automatically infers the orientation (records, columns, index).

**Loading from URL (API):**
```python
df = pd.read_json('https://api.exchangerate-api.com/v4/latest/INR')
```
This fetches live currency exchange rates. The JSON response is parsed directly into a DataFrame.

### 4.2 Working with SQL
We use `mysql.connector` to interface with a MySQL database.

**Setting up the Connection:**
```python
import mysql.connector
conn = mysql.connector.connect(host='localhost', user='root', password='', database='world')
```
This establishes a session with the database server.

**Executing Queries:**
```python
df = pd.read_sql_query("SELECT * FROM countrylanguage", conn)
```
Pandas executes the SQL query and converts the result set (rows and columns) into a DataFrame, preserving column names and data types where possible.

## 5. Summary
This module equips you with the skills to step outside the world of static files and interact with the dynamic systems (APIs and Databases) that power the real world.
