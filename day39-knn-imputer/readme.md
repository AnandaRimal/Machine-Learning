# KNN Imputer (Multivariate Imputation)

## 1. Introduction
Simple imputation (Mean/Median) is "Univariate"â€”it only looks at the column itself. But data columns are often related. If `Age` is missing, maybe `Fare` and `Pclass` can help us guess it? **KNN (K-Nearest Neighbors) Imputer** finds the most similar rows (neighbors) and uses their values to fill the gap.

![Diagram showing a missing point being filled by averaging its nearest neighbors generated by Nano Banana Model](https://via.placeholder.com/800x400?text=KNN+Imputation+Logic+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### How it works
1.  **Distance Metric**: Calculate the Euclidean distance between the row with the missing value and all other rows (using the observed features).
2.  **Find Neighbors**: Select the $K$ closest rows (e.g., 3 most similar passengers).
3.  **Impute**: Calculate the weighted average of the neighbors' values for the missing feature.

### Pros & Cons
- **Pros**: Much more accurate than Mean/Median because it uses the correlation structure of the data.
- **Cons**: Computationally expensive (calculating distances between all pairs of rows). Requires scaling data first.

## 3. Visualizing the Concept
*(Imagine a 2D scatter plot here generated by the Nano Banana Model: A ghost point appears in the center of a cluster of 3 other points)*

## 4. Practical Implementation & Notebook Walkthrough

The notebook `day39.ipynb` demonstrates this on the Titanic dataset.

### 4.1 Setup
**Crucial**: KNN calculates distances, so features with larger scales will dominate. You **must** scale your data (MinMax or Standard) before using KNN Imputer.
*(Note: The notebook might skip explicit scaling for simplicity, but in production, always scale).*

### 4.2 Using `KNNImputer`
```python
from sklearn.impute import KNNImputer

# n_neighbors=3, weights='distance' (closer neighbors have more influence)
knn = KNNImputer(n_neighbors=3, weights='distance')

X_train_trf = knn.fit_transform(X_train)
```

### 4.3 Performance Comparison
The notebook compares Logistic Regression results:
- **Simple Imputer (Mean)**: Lower accuracy.
- **KNN Imputer**: Higher accuracy.
This confirms that using the relationships between features yields better estimates than a blind average.

## 5. Summary
KNN Imputer is a powerful "Multivariate" imputation technique. It's the smart way to fill missing values, provided your dataset isn't too massive (where it becomes too slow). 
