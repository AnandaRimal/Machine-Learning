# Working with CSV Files

## 1. Introduction
CSV (Comma Separated Values) is the lingua franca of data science. It is a simple, text-based format that stores tabular data. Despite its simplicity, real-world CSV files can be messy, inconsistent, and challenging to parse. This chapter dives deep into the `pandas` library's capabilities to handle every nuance of CSV file processing, from basic loading to handling complex delimiters and large datasets.

![Diagram showing the structure of a CSV file and how Pandas parses it into a DataFrame. Generated by Nano Banana Model](https://via.placeholder.com/800x400?text=CSV+Parsing+Visualization+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### What is a CSV?
A CSV file is a plain text file where:
- Each line represents a **row** of data.
- Values within a row are separated by a **delimiter** (usually a comma `,`).
- The first line often contains the **header** (column names).

### The Challenge of Parsing
While the format seems trivial, parsers must handle:
- **Quoting**: Fields containing commas (e.g., "New York, NY") must be quoted.
- **Encoding**: Files may use UTF-8, Latin-1, or CP1252.
- **Missing Data**: How to represent null values (e.g., `NA`, `NULL`, `?`).
- **Data Types**: Inferring if "2023-01-01" is a string or a date.

### Mathematical/Algorithmic Perspective
When `pandas.read_csv()` runs, it performs:
1.  **Tokenization**: Splitting the byte stream based on the delimiter.
2.  **Type Inference**: Sampling the first $N$ rows to guess the data type (int, float, object) for each column.
3.  **Memory Allocation**: Creating NumPy arrays for each column.

#### I/O and Memory Math
- Let $n$ be rows, $m$ columns. For numeric columns (float64): memory $\approx 8\,\text{bytes} \times n$ per column; total $\approx 8nm$ bytes. Downcasting to float32 halves usage.
- Parsing time roughly scales as $\mathcal{O}(\text{file size})$; enabling `usecols` reduces tokenization cost.
- Chunking processes $k$ rows at a time, keeping peak memory bounded by $\mathcal{O}(km)$.

## 3. Visualizing the Concept
*(Imagine a flowchart here generated by the Nano Banana Model showing the data flow from a raw text file -> Buffer -> Tokenizer -> DataFrame)*
![CSV dialects and quoting rules (to be generated by Nano Banana Model)](https://via.placeholder.com/800x400?text=CSV+Dialects+%26+Quoting+by+Nano+Banana+Model)

## 4. Practical Implementation & Notebook Walkthrough

The notebook `working-with-csv.ipynb` is a comprehensive guide to the `read_csv` function. Here is a breakdown of the key techniques demonstrated:

### 4.1 Basic Loading
We start by loading a standard CSV file.
```python
import pandas as pd
df = pd.read_csv('aug_train.csv')
```

### 4.2 Loading from URL
Pandas can accept a URL directly, handling the HTTP request internally.
```python
df = pd.read_csv('https://raw.githubusercontent.com/...')
```

### 4.3 Handling Custom Separators (`sep`)
Not all CSVs use commas. TSV (Tab Separated Values) files use `\t`.
```python
pd.read_csv('movie_titles_metadata.tsv', sep='\t')
```

Other dialects: `sep=';'` (European CSV), `decimal=','` to parse numbers like `1,23`.

### 4.4 Header Management (`header` & `names`)
If a file lacks headers, we can assign them manually.
```python
pd.read_csv('test.csv', header=None, names=['col1', 'col2'])
```

### 4.5 Indexing (`index_col`)
Instead of the default 0, 1, 2 index, we can use a unique identifier column from the dataset.
```python
pd.read_csv('aug_train.csv', index_col='enrollee_id')
```

### 4.6 Filtering Columns (`usecols`)
For large datasets, loading only necessary columns saves memory.
```python
pd.read_csv('aug_train.csv', usecols=['enrollee_id', 'gender', 'target'])
```

### 4.7 Handling Missing Values (`na_values`)
We can specify custom strings that should be treated as NaN.
```python
pd.read_csv('aug_train.csv', na_values=['-', '??', 'NA'])
```

### 4.8 Data Type Optimization (`dtype`)
Manually specifying types reduces memory usage and prevents inference errors.
```python
pd.read_csv('aug_train.csv', dtype={'target': int})
```

Downcasting example:
```python
df = pd.read_csv('aug_train.csv')
df['some_float'] = pd.to_numeric(df['some_float'], downcast='float')  # float32
df['some_int'] = pd.to_numeric(df['some_int'], downcast='integer')    # int32
```

### 4.9 Date Parsing (`parse_dates`)
Converting string dates to datetime objects during load time.
```python
pd.read_csv('ipl.csv', parse_dates=['date'])
```

### 4.10 Processing Large Files (`chunksize`)
For files larger than RAM, we process them in chunks.
```python
dfs = pd.read_csv('aug_train.csv', chunksize=5000)
for chunk in dfs:
    # Process each 5000-row chunk
    print(chunk.shape)
```

Aggregate in streaming fashion:
```python
total = 0
for chunk in pd.read_csv('aug_train.csv', chunksize=100_000, usecols=['target']):
    total += chunk['target'].sum()
```

Robust reading with encodings and errors:
```python
pd.read_csv('zomato.csv', encoding='utf-8', on_bad_lines='skip')
```

Compressed files:
```python
pd.read_csv('data.csv.gz', compression='gzip')
```

Datetime + timezone handling:
```python
pd.read_csv('times.csv', parse_dates=['ts'])
df['ts'] = df['ts'].dt.tz_localize('UTC').dt.tz_convert('Asia/Kathmandu')
```

## 5. Summary
This module transforms you from a user who simply runs `read_csv('file.csv')` into an expert who can handle any text-based data format thrown their way.

## ‚úÖ Advantages and ‚ö†Ô∏è Disadvantages
- **Advantages**: Human-readable, widely supported, stream-friendly, compressible.
- **Disadvantages**: Ambiguous types, dialect inconsistencies, expensive parsing vs binary formats.

## üß≠ When to Use vs Alternatives
- **Use CSV**: Interchange, quick inspection, lightweight ETL.
- **Prefer Parquet/Feather**: Large analytics tables (typed, columnar, faster I/O).

![CSV vs Parquet trade-offs (to be generated by Nano Banana Model)](https://via.placeholder.com/800x400?text=CSV+vs+Parquet+Trade-Offs+by+Nano+Banana+Model)
