# Random Forest (Bagging)

## 1. Introduction
A single Decision Tree is prone to overfitting. It memorizes the training data.
**Random Forest** solves this by training hundreds of trees on random subsets of the data and averaging their predictions. This is called **Bagging (Bootstrap Aggregating)**.
"Wisdom of the Crowd": A crowd of average people is smarter than a single expert.

![Illustration of many small trees voting on a result generated by Nano Banana Model](https://via.placeholder.com/800x400?text=Random+Forest+Voting+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### How it works
1.  **Bootstrapping**: Create $N$ random datasets by sampling with replacement from the original data.
2.  **Feature Randomness**: When splitting a node, only consider a random subset of features (not all). This ensures trees are different (decorrelated).
3.  **Aggregation**:
    - **Classification**: Majority Vote.
    - **Regression**: Average of predictions.

### OOB (Out-of-Bag) Score
Since we sample with replacement, some rows are left out (~37%). We can use these "unseen" rows to validate the model without a separate test set.

### Feature Importance
- **Impurity-based**: Accumulated Gini/entropy decrease across splits using a feature.
- **Permutation Importance**: Measure drop in score after shuffling a feature; more reliable, captures interactions.

### Hyperparameters
- `n_estimators`: More trees â†’ smoother, more stable.
- `max_depth`, `min_samples_split/leaf`: Control complexity.
- `max_features`: Subset size per split; lower values decorrelate trees more.

## 3. Visualizing the Concept
*(Imagine 100 judges looking at a case. Each judge sees slightly different evidence. Their combined verdict is more reliable than any single judge generated by the Nano Banana Model)*

## 4. Practical Implementation & Notebook Walkthrough

The notebook `random_forest_demo.ipynb` compares a Decision Tree vs. Random Forest on a circular dataset.

### 4.1 Decision Tree (Overfitting)
The single tree creates a jagged, complex boundary that tries to capture every noise point.

### 4.2 Random Forest (Smooth)
```python
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=500)
rf.fit(X_train, y_train)
```
The Random Forest boundary is smooth and circular, capturing the true underlying pattern.

### 4.3 OOB and Importance
```python
rf = RandomForestClassifier(n_estimators=500, oob_score=True, random_state=0)
rf.fit(X_train, y_train)
rf.oob_score_

# Permutation importance
from sklearn.inspection import permutation_importance
pi = permutation_importance(rf, X_test, y_test, n_repeats=10)
```

## 5. Summary
Random Forest is one of the most robust and versatile algorithms. It handles non-linear data, requires little tuning, and gives Feature Importance for free.
