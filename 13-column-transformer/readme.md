# Column Transformer - Heterogeneous Data Processing

## General Idea

ColumnTransformer is a scikit-learn utility that applies different transformations to different subsets of features in a dataset. It enables processing heterogeneous data where different columns require different preprocessing strategies (e.g., scaling numeric features while encoding categorical ones) in a single, unified pipeline. This is essential for real-world datasets that contain mixed data types.

## Why Use ColumnTransformer?

1. **Heterogeneous Data Handling**: Apply appropriate transformations to each feature type
2. **Pipeline Integration**: Seamlessly fits into scikit-learn pipelines
3. **Code Organization**: Clean, maintainable preprocessing code
4. **Prevent Leakage**: Ensures consistent train/test transformation
5. **Parallelization**: Can process transformations in parallel
6. **Flexibility**: Mix any combination of transformers
7. **Production Ready**: Easy deployment with single fitted object

## Role in Machine Learning

### Data Preprocessing Pipeline

**Typical ML Workflow**:
1. **Raw Data**: Mixed types (numeric, categorical, text, dates)
2. **ColumnTransformer**: Apply type-specific transformations
3. **Transformed Data**: Numeric matrix ready for model
4. **Model Training**: Feed processed data to estimator

**Example Pipeline**:
```
Numeric features   \u2192 StandardScaler    \u2192\nCategorical       \u2192 OneHotEncoder     \u2192  Combined Matrix \u2192 Model\nOrdinal           \u2192 OrdinalEncoder    \u2192\nText              \u2192 TfidfVectorizer   \u2192\n```\n\n### Preventing Data Leakage\n\n**Problem**: Fitting transformers on entire dataset leaks test information\n\n**Solution**: ColumnTransformer with Pipeline\n- Fit on training data only\n- Transform train and test separately\n- Statistics computed only from training\n\n**Mathematical Guarantee**:\n$$\\mu_{transform} = \\mu_{train}, \\quad \\sigma_{transform} = \\sigma_{train}$$\n\nTest data never influences transformation parameters\n\n## Structure and Syntax\n\n### Basic Structure\n\n**Components**:\n1. **Transformers**: List of (name, transformer, columns) tuples\n2. **Remainder**: How to handle columns not specified\n3. **n_jobs**: Parallelization parameter\n\n**Syntax**:\n```python\nColumnTransformer(\n    transformers=[\n        ('name1', transformer1, column_selector1),\n        ('name2', transformer2, column_selector2),\n        ...\n    ],\n    remainder='drop',  # 'drop', 'passthrough', or transformer\n    n_jobs=None,       # Number of parallel jobs\n    verbose=False\n)\n```\n\n### Column Selection Methods\n\n**1. By Name** (list of column names):\n```python\ncolumns=['age', 'income', 'score']\n```\n\n**2. By Index** (list of integers):\n```python\ncolumns=[0, 2, 5]\n```\n\n**3. By Boolean Mask**:\n```python\ncolumns=[True, False, True, False]  # Select 1st and 3rd\n```\n\n**4. By Callable** (function returning column selector):\n```python\ndef select_numeric(X):\n    return X.select_dtypes(include=['number']).columns\n\ncolumns=select_numeric\n```\n\n**5. Using make_column_selector**:\n```python\nfrom sklearn.compose import make_column_selector\n\ncolumns=make_column_selector(dtype_include='number')\ncolumns=make_column_selector(pattern='price.*')\n```\n\n## Mathematical Representation\n\n### Transformation Function\n\nFor dataset $X$ with column subsets $C_1, C_2, ..., C_k$:\n\n$$X = [X_{C_1} | X_{C_2} | ... | X_{C_k}]$$\n\nColumnTransformer applies:\n\n$$T(X) = [T_1(X_{C_1}) | T_2(X_{C_2}) | ... | T_k(X_{C_k})]$$\n\nWhere $T_i$ is the transformer for column subset $C_i$\n\n**Result**: Horizontal concatenation of transformed subsets\n\n### Dimensionality\n\n**Input**: $X \\in \\mathbb{R}^{n \\times p}$ (n samples, p features)\n\n**Output**: $T(X) \\in \\mathbb{R}^{n \\times p'}$\n\nWhere $p'$ depends on transformations:\n- **Scaling**: $p' = p$ (same dimensions)\n- **One-hot encoding**: $p' > p$ (dimension increase)\n- **PCA**: $p' < p$ (dimension reduction)\n\n## Common Transformation Scenarios\n\n### Scenario 1: Numeric + Categorical\n\n**Dataset**:\n- Numeric: age, income, credit_score\n- Categorical: gender, education, city\n\n**Transformations**:\n```python\ntransformers=[\n    ('num', StandardScaler(), ['age', 'income', 'credit_score']),\n    ('cat', OneHotEncoder(), ['gender', 'education', 'city'])\n]\n```\n\n**Process**:\n1. Numeric: $X_{num}' = \\frac{X_{num} - \\mu}{\\sigma}$\n2. Categorical: $X_{cat}' = \\text{OneHot}(X_{cat})$\n3. Concatenate: $X' = [X_{num}' | X_{cat}']$\n\n### Scenario 2: Mixed Scaling Strategies\n\n**Dataset**:\n- Normal distribution: height, weight\n- Skewed: income, house_price\n- Binary: has_insurance, is_employed\n\n**Transformations**:\n```python\ntransformers=[\n    ('standard', StandardScaler(), ['height', 'weight']),\n    ('robust', RobustScaler(), ['income', 'house_price']),\n    ('passthrough', 'passthrough', ['has_insurance', 'is_employed'])\n]\n```\n\n### Scenario 3: Multiple Encoding Types\n\n**Dataset**:\n- Ordinal: education_level, company_size\n- Nominal: color, country\n- High-cardinality: zip_code\n\n**Transformations**:\n```python\ntransformers=[\n    ('ordinal', OrdinalEncoder(), ['education_level', 'company_size']),\n    ('onehot', OneHotEncoder(), ['color', 'country']),\n    ('target', TargetEncoder(), ['zip_code'])\n]\n```\n\n### Scenario 4: Feature Engineering Pipeline\n\n**Dataset with derived features**:\n\n**Transformations**:\n```python\nfrom sklearn.preprocessing import FunctionTransformer\n\ndef log_transform(X):\n    return np.log1p(X)\n\ntransformers=[\n    ('log', FunctionTransformer(log_transform), ['price', 'volume']),\n    ('poly', PolynomialFeatures(degree=2), ['length', 'width']),\n    ('scale', StandardScaler(), ['age', 'score'])\n]\n```\n\n## Remainder Parameter\n\n**Purpose**: Specifies what to do with columns not mentioned in transformers\n\n### Options\n\n**1. 'drop'** (default):\n- Remove unspecified columns\n- Only keep explicitly transformed columns\n\n**Use**: When you know all relevant columns\n\n**2. 'passthrough'**:\n- Include unspecified columns unchanged\n- Useful for columns already preprocessed\n\n**Use**: When some columns need no transformation\n\n**3. Custom Transformer**:\n- Apply specific transformation to remainder\n- Example: `remainder=StandardScaler()`\n\n**Use**: Common transformation for many columns\n\n### Example\n\n**Dataset**: [A, B, C, D, E]\n\n**Transformers**: Only specify [A, B]\n\n**Results**:\n```python\nremainder='drop'        \u2192 Output: [A', B']\nremainder='passthrough' \u2192 Output: [A', B', C, D, E]\nremainder=Scaler()      \u2192 Output: [A', B', C', D', E']\n```\n\n## Pipeline Integration\n\n### ColumnTransformer in Pipeline\n\n**Full ML Pipeline**:\n```python\nPipeline([\n    ('preprocessor', ColumnTransformer(...)),\n    ('classifier', LogisticRegression())\n])\n```\n\n**Workflow**:\n1. **Fit**: \n   - ColumnTransformer learns transformation parameters from X_train\n   - Model trains on transformed X_train\n   \n2. **Predict**:\n   - ColumnTransformer transforms X_test using learned parameters\n   - Model predicts on transformed X_test\n\n**Advantages**:\n- Single `fit()` call for entire pipeline\n- Automatic train/test consistency\n- Easy cross-validation\n- Simple deployment (save single pipeline object)\n\n### Nested Pipelines\n\n**Complex preprocessing per column type**:\n```python\nColumnTransformer([\n    ('num_pipeline', Pipeline([\n        ('imputer', SimpleImputer(strategy='mean')),\n        ('scaler', StandardScaler())\n    ]), numeric_features),\n    \n    ('cat_pipeline', Pipeline([\n        ('imputer', SimpleImputer(strategy='most_frequent')),\n        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n    ]), categorical_features)\n])\n```\n\n**Benefits**:\n- Multi-step transformations per column type\n- Clean, modular code\n- Each type gets appropriate preprocessing sequence\n\n## Parallelization\n\n### n_jobs Parameter\n\n**Purpose**: Run transformers in parallel\n\n**Values**:\n- `n_jobs=None` or `n_jobs=1`: Sequential (default)\n- `n_jobs=2`: Use 2 CPU cores\n- `n_jobs=-1`: Use all available cores\n\n**Speedup**:\n$$\\text{Speedup} \\approx \\min(k, \\text{num\\_cores})$$\n\nWhere $k$ is number of transformers\n\n**When Beneficial**:\n- Many independent transformers\n- Large datasets\n- Expensive transformations (e.g., TfidfVectorizer)\n\n**When NOT Beneficial**:\n- Few/simple transformers (overhead > benefit)\n- Small datasets\n- Already parallel downstream (e.g., parallel tree building)\n\n**Example**:\n```python\nColumnTransformer(\n    transformers=[...],\n    n_jobs=-1  # Use all cores\n)\n```\n\n## Feature Names and Column Order\n\n### get_feature_names_out()\n\n**Purpose**: Retrieve output column names after transformation\n\n**Usage**:\n```python\nct = ColumnTransformer([...])\nct.fit(X)\nfeature_names = ct.get_feature_names_out()\n```\n\n**Result**: Array of feature names\n- Original names for passthrough/scaling\n- Generated names for one-hot: `category__value`\n- Numbered for unnamed: `transformer0`, `transformer1`, ...\n\n### Column Order\n\n**Output order**: Same as specification in transformers list\n\n**Example**:\n```python\ntransformers=[\n    ('cat', OneHotEncoder(), ['A']),\n    ('num', StandardScaler(), ['B', 'C'])\n]\n```\n\n**Output**: `[A_encoded_cols, B_scaled, C_scaled]`\n\n**Importance**: Know order for interpretation and debugging\n\n## Sparse vs Dense Output\n\n### Sparse Matrices\n\n**When Generated**:\n- OneHotEncoder produces sparse output\n- TfidfVectorizer produces sparse output\n- Other transformers may support sparse\n\n**Automatic Handling**:\n- ColumnTransformer preserves sparsity\n- Concatenates sparse matrices efficiently\n- Result is sparse if any transformer produces sparse\n\n**Force Dense**:\n```python\nct = ColumnTransformer(\n    transformers=[...],\n    sparse_threshold=0  # Force dense output\n)\n```\n\n**Default**: `sparse_threshold=0.3`\n- If >30% of data would be non-zero, output is dense\n- Otherwise, output is sparse\n\n### Memory Considerations\n\n**Dense Matrix**: $n \\times p' \\times \\text{sizeof}(\\text{float})$\n- Example: 1M rows, 1000 features, float64 \u2192 8GB\n\n**Sparse Matrix**: Much smaller for high sparsity\n- Example: Same data with 90% zeros \u2192 ~0.8GB\n\n**Best Practice**: Keep sparse when possible\n- Many models support sparse input\n- Significant memory savings\n\n## Common Patterns and Best Practices\n\n### Pattern 1: Separate Imputation and Scaling\n\n**Why**: Missing value handling before scaling\n\n```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\nnumeric_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\nColumnTransformer([\n    ('num', numeric_transformer, numeric_features)\n])\n```\n\n### Pattern 2: Automatic Column Selection\n\n**Why**: Adapt to different datasets\n\n```python\nfrom sklearn.compose import make_column_selector\n\nColumnTransformer([\n    ('num', StandardScaler(), make_column_selector(dtype_include='number')),\n    ('cat', OneHotEncoder(), make_column_selector(dtype_include='object'))\n])\n```\n\n### Pattern 3: Custom Transformers\n\n**Why**: Domain-specific transformations\n\n```python\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass DateFeatureExtractor(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        # Extract year, month, day, etc.\n        return extracted_features\n\nColumnTransformer([\n    ('dates', DateFeatureExtractor(), ['purchase_date', 'signup_date'])\n])\n```\n\n### Pattern 4: Conditional Transformation\n\n**Why**: Different transformations for different value ranges\n\n```python\ndef select_high_values(X):\n    return X.columns[X.max() > 1000]\n\ndef select_low_values(X):\n    return X.columns[X.max() <= 1000]\n\nColumnTransformer([\n    ('robust', RobustScaler(), select_high_values),\n    ('standard', StandardScaler(), select_low_values)\n])\n```\n\n## Debugging and Inspection\n\n### Accessing Fitted Transformers\n\n**After fitting**:\n```python\nct.named_transformers_['transformer_name']\n```\n\n**Use**: Inspect learned parameters\n\n**Example**:\n```python\n# Get scaling parameters\nscaler = ct.named_transformers_['num']\nmean = scaler.mean_\nstd = scaler.scale_\n```\n\n### Visualizing Transformations\n\n**Check transformed data**:\n```python\nX_transformed = ct.transform(X)\nprint(X_transformed.shape)\nprint(ct.get_feature_names_out())\n```\n\n**Verify**:\n- Correct number of features\n- Expected feature names\n- No NaN/inf values\n- Appropriate scales\n\n### Common Issues\n\n**1. Shape Mismatch**:\n- Check column selectors don't overlap\n- Verify all specified columns exist\n\n**2. Memory Error**:\n- Use sparse matrices\n- Process in batches\n- Reduce feature count\n\n**3. Unexpected Output**:\n- Check remainder parameter\n- Verify transformer order\n- Use get_feature_names_out()\n\n**4. Slow Performance**:\n- Enable parallelization (n_jobs=-1)\n- Simplify transformations\n- Sample data for experimentation\n\n## Comparison with Alternatives\n\n### Manual Transformation\n\n**Without ColumnTransformer**:\n```python\nX_num_scaled = scaler.fit_transform(X[numeric_cols])\nX_cat_encoded = encoder.fit_transform(X[categorical_cols])\nX_transformed = np.hstack([X_num_scaled, X_cat_encoded])\n```\n\n**Issues**:\n- Verbose\n- Error-prone (manual concatenation)\n- Harder to integrate in pipeline\n- Must track transformers separately\n\n**With ColumnTransformer**: Single object, automatic concatenation\n\n### FeatureUnion\n\n**Difference**: \n- **ColumnTransformer**: Operates on column subsets\n- **FeatureUnion**: Operates on entire dataset, concatenates results\n\n**Use FeatureUnion when**: \n- Same columns, different transformations\n- Want multiple representations\n\n**Use ColumnTransformer when**:\n- Different columns, different transformations\n- Heterogeneous data\n\n## Mathematical Example\n\n### Setup\n\n**Dataset**: 3 features [Age, Income, Color]\n```\nAge    Income    Color\n25     50000     Red\n30     60000     Blue\n35     70000     Red\n```\n\n**Transformations**:\n- Age, Income: StandardScaler\n- Color: OneHotEncoder\n\n### Step-by-Step\n\n**1. Numeric Transformation**:\n\nAge: $\\mu = 30, \\sigma = 5$\n$$\\text{Age}' = \\frac{[25, 30, 35] - 30}{5} = [-1, 0, 1]$$\n\nIncome: $\\mu = 60000, \\sigma = 10000$\n$$\\text{Income}' = \\frac{[50000, 60000, 70000] - 60000}{10000} = [-1, 0, 1]$$\n\n**2. Categorical Transformation**:\n\nColor: [Red, Blue] \u2192 One-hot\n```\nRed  \u2192 [1, 0]\nBlue \u2192 [0, 1]\nRed  \u2192 [1, 0]\n```\n\n**3. Concatenation**:\n\n$$X' = \\begin{bmatrix}\n-1 & -1 & 1 & 0 \\\\\n0 & 0 & 0 & 1 \\\\\n1 & 1 & 1 & 0\n\\end{bmatrix}$$\n\nColumns: [Age', Income', Color_Red, Color_Blue]\n\n## Summary\n\nColumnTransformer is an essential tool for preprocessing heterogeneous data in machine learning pipelines. It enables applying different transformations to different column subsets in a clean, maintainable, and pipeline-compatible way.\n\n**Key Concepts**:\n\n**Core Functionality**:\n- Apply transformers to column subsets\n- Automatically concatenate results\n- Integrate seamlessly with Pipeline\n\n**Mathematical Operation**:\n$$T(X) = [T_1(X_{C_1}) | T_2(X_{C_2}) | ... | T_k(X_{C_k})]$$\n\n**Column Selection**:\n- By name, index, boolean mask, callable\n- make_column_selector for automatic selection\n\n**Remainder Handling**:\n- 'drop': Remove unspecified columns\n- 'passthrough': Keep unchanged\n- Custom transformer: Apply specific transformation\n\n**Best Practices**:\n- Use with Pipeline for full workflow\n- Leverage parallelization for large datasets\n- Nest pipelines for multi-step transformations per type\n- Keep sparse matrices when possible\n- Use make_column_selector for flexibility\n- Access fitted transformers for inspection\n\n**Advantages**:\n- Clean, organized preprocessing\n- Prevents data leakage\n- Easy deployment (single object)\n- Supports parallelization\n- Automatic feature concatenation\n\n**Common Use Cases**:\n- Mixed numeric/categorical data\n- Different scaling strategies per feature group\n- Multiple encoding types\n- Complex preprocessing workflows\n\nMastering ColumnTransformer is crucial for building robust, production-ready machine learning pipelines that handle real-world heterogeneous datasets effectively.