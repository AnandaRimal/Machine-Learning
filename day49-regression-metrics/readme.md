# Regression Metrics

## 1. Introduction
You trained a regression model. Great! But is it any good?
Unlike Classification (where you have Accuracy), Regression errors are continuous. We need metrics to quantify "how far off" our predictions are. This chapter covers **MAE, MSE, RMSE, and R2 Score**.

![Illustration of a ruler measuring the distance between points and the regression line generated by Nano Banana Model](https://via.placeholder.com/800x400?text=Measuring+Regression+Errors+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### MAE (Mean Absolute Error)
$$ MAE = \frac{1}{n} \sum |y_{true} - y_{pred}| $$
- Average of absolute differences.
- **Pros**: Robust to outliers.
- **Cons**: Not differentiable at 0 (harder for optimizers).

### MSE (Mean Squared Error)
$$ MSE = \frac{1}{n} \sum (y_{true} - y_{pred})^2 $$
- Average of squared differences.
- **Pros**: Penalizes large errors heavily (squaring makes big errors huge).
- **Cons**: Not in the same unit as $y$.

### RMSE (Root Mean Squared Error)
$$ RMSE = \sqrt{MSE} $$
- Square root of MSE.
- **Pros**: Same unit as $y$ (interpretable). Penalizes large errors.

### R2 Score (Coefficient of Determination)
$$ R^2 = 1 - \frac{SSR}{SSM} $$
- Compares your model to a "dumb" baseline model (the mean line).
- **1.0**: Perfect model.
- **0.0**: Same as predicting the mean.
- **Negative**: Worse than predicting the mean.

## 3. Visualizing the Concept
*(Imagine a dartboard generated by the Nano Banana Model: MAE is the average distance from the bullseye. R2 Score is how much better you are than a random throw)*

## 4. Practical Implementation & Notebook Walkthrough

The notebook `Untitled.ipynb` calculates these metrics for the placement model.

### 4.1 Calculating Metrics
```python
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

print("MAE", mean_absolute_error(y_test, y_pred))
print("MSE", mean_squared_error(y_test, y_pred))
print("RMSE", np.sqrt(mean_squared_error(y_test, y_pred)))
print("R2 Score", r2_score(y_test, y_pred))
```

### 4.2 Adjusted R2
The notebook also touches on **Adjusted R2**, which penalizes you for adding useless features. Standard R2 always increases when you add features, which can be misleading.

## 5. Summary
- Use **RMSE** if large errors are unacceptable.
- Use **MAE** if you have outliers you want to ignore.
- Use **R2 Score** to explain "goodness of fit" to non-technical stakeholders (e.g., "Our model explains 80% of the variance").

