# Types of Gradient Descent

## 1. Introduction
In standard Gradient Descent, we calculate the error for **all** data points before taking a single step. If you have 10 million rows, that's 10 million calculations for *one* step. That's too slow.
This chapter introduces variations: **Batch**, **Stochastic**, and **Mini-Batch** Gradient Descent.

![Illustration of three paths down a mountain: one smooth (Batch), one zigzag (Stochastic), and one in between (Mini-Batch) generated by Nano Banana Model](https://via.placeholder.com/800x400?text=GD+Variations+Paths+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### Batch Gradient Descent
- **Method**: Use the entire dataset to calculate the gradient.
- **Pros**: Smooth convergence, stable.
- **Cons**: Extremely slow for large datasets. Memory intensive.

### Stochastic Gradient Descent (SGD)
- **Method**: Pick **one** random row, calculate error, update weights. Repeat.
- **Pros**: Blazing fast.
- **Cons**: Noisy convergence (zig-zags around the minimum).

### Mini-Batch Gradient Descent
- **Method**: Pick a small batch (e.g., 32 rows), calculate error, update weights.
- **Pros**: Best of both worlds. Stable enough, fast enough.
- **Standard**: This is the default for Deep Learning.

## 3. Visualizing the Concept
*(Imagine a race: Batch GD is a slow, steady giant. SGD is a drunk sprinter running in zig-zags but getting there fast. Mini-Batch is a coordinated team generated by the Nano Banana Model)*

## 4. Practical Implementation & Notebook Walkthrough

The notebooks implement these from scratch.

### 4.1 Batch GD (`batch-gradient-descent.ipynb`)
The loop iterates over `epochs`. Inside, it uses `X_train` (all data).
```python
# Vectorized implementation
y_hat = np.dot(X, coef) + intercept
der = -2 * np.dot(y - y_hat, X) / n
```

### 4.2 Stochastic GD (`stochastic-gradient-descent-from-scratch.ipynb`)
The loop iterates over `epochs`, and inside that, it iterates over **every row**.
```python
for i in range(X.shape[0]): # Loop over rows
    # Update weights based on single row index i
```

### 4.3 Mini-Batch GD
We shuffle the data and split it into batches of size `batch_size`.
```python
for i in range(0, X.shape[0], batch_size):
    # Update weights based on slice [i : i+batch_size]
```

## 5. Summary
- **Small Data**: Use Batch GD.
- **Big Data**: Use Mini-Batch GD.
- **SGD**: Rarely used in its pure form, but the concept powers advanced optimizers like Adam and RMSprop.
