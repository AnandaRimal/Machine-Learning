# Classification Metrics

## 1. Introduction
In Regression, we measured "distance" (MSE). In Classification, we measure "correctness". But "Accuracy" isn't enough.
If 99% of your patients don't have cancer, a model that says "No Cancer" for everyone has 99% accuracy but is useless. We need sharper tools: **Precision, Recall, and F1-Score**.

![Illustration of a Confusion Matrix with True Positives, False Positives, True Negatives, and False Negatives generated by Nano Banana Model](https://via.placeholder.com/800x400?text=Confusion+Matrix+Visualized+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### The Confusion Matrix
A 2x2 table showing:
- **TP (True Positive)**: Correctly predicted Yes.
- **TN (True Negative)**: Correctly predicted No.
- **FP (False Positive)**: Predicted Yes, but actually No (Type I Error).
- **FN (False Negative)**: Predicted No, but actually Yes (Type II Error).

### The Metrics
- **Accuracy**: $(TP+TN) / Total$. Good for balanced data.
- **Precision**: $TP / (TP+FP)$. "Of all the ones I said were Yes, how many were actually Yes?" (Crucial for Spam Detection).
- **Recall (Sensitivity)**: $TP / (TP+FN)$. "Of all the actual Yeses, how many did I find?" (Crucial for Cancer Detection).
- **F1 Score**: Harmonic mean of Precision and Recall. $2 \times (P \times R) / (P + R)$.

### Thresholds, ROC, and PR Curves
- **Decision Threshold**: Converting probabilities to classes (default 0.5) affects Precision/Recall trade‑off.
- **ROC Curve**: TPR vs FPR across thresholds; **AUC** summarizes the curve (chance=0.5, perfect=1.0).
- **PR Curve**: Precision vs Recall; preferred for highly imbalanced data where ROC can be overly optimistic.

## 3. Visualizing the Concept
*(Imagine a net fishing for tuna. Precision is "how many fish in the net are actually tuna". Recall is "how many of the ocean's tuna did we catch" generated by the Nano Banana Model)*

## 4. Practical Implementation & Notebook Walkthrough

The notebook `classification-metrics-binary.ipynb` compares models on the Heart Disease dataset.

### 4.1 Calculating Accuracy
```python
from sklearn.metrics import accuracy_score

print("Accuracy:", accuracy_score(y_test, y_pred))
```

### 4.2 Confusion Matrix
```python
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
```
This outputs the raw counts of TP, TN, FP, FN.

### 4.3 Curves and AUC
```python
from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, average_precision_score

y_proba = clf.predict_proba(X_test)[:,1]
fpr, tpr, thr = roc_curve(y_test, y_proba)
auc = roc_auc_score(y_test, y_proba)
prec, rec, thr_pr = precision_recall_curve(y_test, y_proba)
ap = average_precision_score(y_test, y_proba)
```
Plot curves to visualize trade‑offs and pick thresholds aligned with business costs.

## 5. Summary
- **Balanced Data**: Accuracy is fine.
- **Imbalanced Data**: Never use Accuracy. Use F1-Score.
- **High Cost of False Positive**: Optimize Precision.
- **High Cost of False Negative**: Optimize Recall.
 - Use **ROC/AUC** for general separability; **PR/AP** for imbalanced cases.
