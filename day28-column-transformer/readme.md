# Column Transformer

## 1. Introduction
In real-world datasets, different columns require different preprocessing. You might want to impute missing values in `Age`, One-Hot Encode `City`, and Ordinal Encode `Education`. Doing this manually (splitting, transforming, concatenating) is messy and error-prone. **ColumnTransformer** is the "Mentos Zindagi" (Smart Life) way to handle this elegantly.

![Diagram showing multiple columns entering a transformer and exiting processed generated by Nano Banana Model](https://via.placeholder.com/800x400?text=Column+Transformer+Workflow+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### The Problem: Heterogeneous Data
A dataset usually contains:
- Numerical columns (need Scaling/Imputation).
- Nominal columns (need One-Hot Encoding).
- Ordinal columns (need Ordinal Encoding).

### The Solution: Parallel Processing
`ColumnTransformer` allows you to apply different transformers to different subsets of columns **in parallel** and then automatically concatenates the results.

**Pros/Cons**
- Pros: Single source of truth for preprocessing; avoids leakage; reproducible.
- Cons: Debugging transformed column names can be tricky; high-cardinality OHE can explode dimensionality.

## 3. Visualizing the Concept
*(Imagine a factory conveyor belt here generated by the Nano Banana Model: Raw mixed items enter -> Robot Arms pick specific items -> Process them -> Place them back on a single clean belt)*

## 4. Practical Implementation & Notebook Walkthrough

The notebook `day28.ipynb` compares the "Hard Way" vs. the "Smart Way".

### 4.1 The Hard Way (Aam Zindagi)
1.  Isolate `fever` column -> Apply `SimpleImputer`.
2.  Isolate `cough` column -> Apply `OrdinalEncoder`.
3.  Isolate `gender`, `city` -> Apply `OneHotEncoder`.
4.  Isolate `age` -> Keep as is.
5.  `np.concatenate` all 4 resulting arrays.
*Result*: Messy code, hard to maintain, easy to mess up column order.

### 4.2 The Smart Way (Mentos Zindagi)
We define a single object that does it all.
```python
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression

transformer = ColumnTransformer(transformers=[
    ('tnf1', SimpleImputer(), ['fever']),
    ('tnf2', OrdinalEncoder(categories=[['Mild','Strong']]), ['cough']),
    ('tnf3', OneHotEncoder(sparse=False, drop='first'), ['gender','city'])
], remainder='passthrough') # 'passthrough' keeps the other columns (age)

pipe = Pipeline([
    ('pre', transformer),
    ('model', LogisticRegression(max_iter=1000))
])
```

### 4.3 Execution
```python
X_train_transformed = transformer.fit_transform(X_train)
X_test_transformed = transformer.transform(X_test)
pipe.fit(X_train, y_train)
print(pipe.score(X_test, y_test))
```
One line of code handles everything.

## 5. Summary
`ColumnTransformer` is essential for clean, professional ML code. Use it inside Pipelines to bind preprocessing with modeling and prevent train/test leakage.

