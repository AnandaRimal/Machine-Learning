# Binning & Binarization

## 1. Introduction
Sometimes, the exact value of a number matters less than the "bucket" it falls into. Is a person aged 32 significantly different from 33? Probably not. But "Young Adult" vs "Senior" is a meaningful distinction. **Binning** (Discretization) converts continuous numbers into categories. **Binarization** converts them into just two categories (0 or 1).

![Illustration of continuous data being sorted into bins generated by Nano Banana Model](https://via.placeholder.com/800x400?text=Binning+Process+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### Binning (Discretization)
Grouping continuous values into intervals.
- **Unsupervised Binning**:
    - **Uniform**: Equal width bins (e.g., 0-10, 10-20, 20-30).
    - **Quantile**: Equal frequency bins (e.g., Top 25%, Bottom 25%).
    - **K-Means**: Uses clustering to find natural groups.
- **Supervised Binning**: Uses the target variable to find optimal split points (Decision Trees do this internally).

**Math**
- Quantile bins: cut points at $Q_{p}$ for $p \in \{\frac{1}{k},\frac{2}{k},...,\frac{k-1}{k}\}$ to equalize bin frequencies.
- Entropy-based supervised splits: choose threshold $t$ minimizing impurity (e.g., Gini/Entropy) on left/right partitions.

### Binarization
A special case of binning with only 2 bins.
- Example: Converting "Probability" to "Pass/Fail" based on a threshold of 0.5.

## 3. Visualizing the Concept
*(Imagine a histogram here generated by the Nano Banana Model: The continuous curve is sliced into vertical bars, representing the bins)*

## 4. Practical Implementation & Notebook Walkthrough

The notebook `day32.ipynb` uses the Titanic dataset.

### 4.1 Binning with `KBinsDiscretizer`
We use `sklearn.preprocessing.KBinsDiscretizer`.
```python
from sklearn.preprocessing import KBinsDiscretizer

# n_bins=10, encode='ordinal', strategy='quantile'
kbin = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')

X_train_trf = kbin.fit_transform(X_train)
```
*Impact*: Binning makes the model robust to outliers and noise. It can turn a non-linear relationship into a linear one (for the model).

Integrate with Pipeline:
```python
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

numeric = ['Age','Fare']
pre = ColumnTransformer([
    ('bin', KBinsDiscretizer(n_bins=5, encode='onehot-dense', strategy='quantile'), numeric)
], remainder='drop')

pipe = Pipeline([
    ('pre', pre),
    ('model', LogisticRegression(max_iter=1000))
])
pipe.fit(X_train, y_train)
print(pipe.score(X_test, y_test))
```

### 4.2 Binarization with `Binarizer`
We use `sklearn.preprocessing.Binarizer`.
```python
from sklearn.preprocessing import Binarizer

# threshold=0.5 means values <= 0.5 become 0, > 0.5 become 1
trf = Binarizer(threshold=0.5)

X_train_trf = trf.fit_transform(X_train)
```
*Use Case*: Useful for image processing (thresholding) or creating binary flags from probabilities.

```python
# Example: flag high fare
from sklearn.preprocessing import Binarizer
trf = Binarizer(threshold=df['Fare'].median())
df['Fare_high'] = trf.transform(df[['Fare']])
```

## 5. Summary
Binning is a form of **smoothing**. It reduces noise by sacrificing precision and can linearize non-linear relationships for simple models. Prefer quantile bins for robustness and integrate via Pipelines for reproducibility.
