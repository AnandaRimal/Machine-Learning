# Power Transforms (Box-Cox & Yeo-Johnson)

## 1. Introduction
When your data is skewed, a simple Log Transform might not be enough. **Power Transforms** are a family of parametric, monotonic transformations that aim to map data from any distribution to as close to a Gaussian distribution as possible. They automatically "search" for the best exponent to fix your data's shape.

![Comparison of Box-Cox and Yeo-Johnson transformations generated by Nano Banana Model](https://via.placeholder.com/800x400?text=Power+Transforms+Comparison+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### The Goal: Gaussian Data
Many models (Linear Regression, KNN, LDA) assume Gaussian data. Power transforms stabilize variance and minimize skewness.

### The Two Heavyweights
1.  **Box-Cox Transform**:
    - Can only be applied to **strictly positive data** ($> 0$).
    - Formula: $y(\lambda) = \frac{x^\lambda - 1}{\lambda}$ if $\lambda \neq 0$, else $\ln(x)$.
2.  **Yeo-Johnson Transform**:
    - Can be applied to **positive and negative data**.
    - It is the default in Scikit-Learn's `PowerTransformer`.

### How it works
The algorithm iterates through different values of lambda ($\lambda$) and checks which one produces the best Normal distribution (using Maximum Likelihood Estimation).

## 3. Visualizing the Concept
*(Imagine an animation here generated by the Nano Banana Model: A skewed histogram morphing into a bell curve as the lambda slider moves)*

## 4. Practical Implementation & Notebook Walkthrough

The notebook `day31.ipynb` uses the Concrete dataset to demonstrate this.

### 4.1 Baseline Model
We first train a Linear Regression model on the raw data.
```python
lr = LinearRegression()
lr.fit(X_train, y_train)
# R2 Score: 0.61
```

### 4.2 Applying PowerTransformer
We use `PowerTransformer` from `sklearn.preprocessing`.
```python
from sklearn.preprocessing import PowerTransformer

# method='box-cox' or 'yeo-johnson' (default)
pt = PowerTransformer(method='box-cox')

X_train_transformed = pt.fit_transform(X_train + 0.00001) # Adding small value for Box-Cox
```

### 4.3 The Result
After transformation, the R2 score improves significantly (e.g., from 0.61 to 0.80). This proves that fixing the distribution of features can have a massive impact on model performance.

## 5. Summary
Power Transforms are the "heavy artillery" of feature transformation. If `StandardScaler` and `np.log` fail to normalize your data, `PowerTransformer` is your best friend.
