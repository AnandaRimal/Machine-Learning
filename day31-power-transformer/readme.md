# Power Transforms (Box-Cox & Yeo-Johnson)

## 1. Introduction
When your data is skewed, a simple Log Transform might not be enough. **Power Transforms** are a family of parametric, monotonic transformations that aim to map data from any distribution to as close to a Gaussian distribution as possible. They automatically "search" for the best exponent to fix your data's shape.

![Comparison of Box-Cox and Yeo-Johnson transformations generated by Nano Banana Model](https://via.placeholder.com/800x400?text=Power+Transforms+Comparison+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### The Goal: Gaussian Data
Many models (Linear Regression, KNN, LDA) assume Gaussian data. Power transforms stabilize variance and minimize skewness.

### The Two Heavyweights
1.  **Box-Cox Transform**:
    - Can only be applied to **strictly positive data** ($> 0$).
    - Formula: $y(\lambda) = \frac{x^\lambda - 1}{\lambda}$ if $\lambda \neq 0$, else $\ln(x)$.
2.  **Yeo-Johnson Transform**:
    - Can be applied to **positive and negative data**.
    - It is the default in Scikit-Learn's `PowerTransformer`.

**Math**
- Box-Cox: $y(\lambda) = \frac{x^\lambda - 1}{\lambda}$ for $\lambda \ne 0$; $y(0)=\ln x$.
- Yeo-Johnson generalizes to $x\le 0$; selects $\lambda$ via MLE optimizing Gaussian likelihood.

### How it works
The algorithm iterates through different values of lambda ($\lambda$) and checks which one produces the best Normal distribution (using Maximum Likelihood Estimation).

## 3. Visualizing the Concept
*(Imagine an animation here generated by the Nano Banana Model: A skewed histogram morphing into a bell curve as the lambda slider moves)*

## 4. Practical Implementation & Notebook Walkthrough

The notebook `day31.ipynb` uses the Concrete dataset to demonstrate this.

### 4.1 Baseline Model
We first train a Linear Regression model on the raw data.
```python
lr = LinearRegression()
lr.fit(X_train, y_train)
# R2 Score: 0.61
```

### 4.2 Applying PowerTransformer
We use `PowerTransformer` from `sklearn.preprocessing`.
```python
from sklearn.preprocessing import PowerTransformer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression

# method='box-cox' or 'yeo-johnson' (default)
pt = PowerTransformer(method='box-cox')

X_train_transformed = pt.fit_transform(X_train + 0.00001) # Adding small value for Box-Cox

pre = ColumnTransformer([
    ('power', PowerTransformer(method='yeo-johnson'), X.columns)
])

pipe = Pipeline([
    ('pre', pre),
    ('model', LinearRegression())
])
pipe.fit(X_train, y_train)
print(pipe.score(X_test, y_test))
```

### 4.3 The Result
After transformation, the R2 score improves significantly (e.g., from 0.61 to 0.80). This proves that fixing the distribution of features can have a massive impact on model performance.

## 5. Summary
Power Transforms are the "heavy artillery" of feature transformation. Use Yeo-Johnson for general data (including non-positive), Box-Cox for positive-only, and integrate via Pipelines to ensure reproducibility and avoid leakage.
