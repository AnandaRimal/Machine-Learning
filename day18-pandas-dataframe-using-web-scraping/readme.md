# Web Scraping to DataFrame

## 1. Introduction
When data is not available via CSV or API, **Web Scraping** is the final frontier. It involves programmatically visiting websites and extracting information from the HTML structure. This chapter covers using `BeautifulSoup` to scrape company data from AmbitionBox and structuring it into a Pandas DataFrame.

![Illustration of a Web Scraper bot extracting data from HTML tags generated by Nano Banana Model](https://via.placeholder.com/800x400?text=Web+Scraping+Process+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### The DOM (Document Object Model)
Webpages are structured as a tree of objects (HTML tags).
- `<html>` -> `<body>` -> `<div>` -> `<p>`
- **Classes and IDs**: CSS selectors used to style elements are also used by scrapers to locate specific data points (e.g., `class="company-rating"`).

### The Scraping Ethics
- **User-Agent**: Identifying your bot to the server.
- **Robots.txt**: Checking if the site allows scraping.
- **Rate Limiting**: Not overwhelming the server with requests.

### The Pipeline
1.  **Fetch**: Get the raw HTML string using `requests`.
2.  **Parse**: Convert the string into a navigable tree using `BeautifulSoup`.
3.  **Extract**: Find specific tags and get their text content.
4.  **Store**: Append to lists and convert to DataFrame.

## 3. Visualizing the Concept
*(Imagine a diagram here generated by the Nano Banana Model showing the HTML source code on one side and the extracted clean table on the other)*

## 4. Practical Implementation & Notebook Walkthrough

The notebook `day18.ipynb` scrapes company reviews from **AmbitionBox**.

### 4.1 Spoofing the User-Agent
Websites often block Python's default User-Agent. We must pretend to be a browser.
```python
headers = {'User-Agent': 'Mozilla/5.0 ...'}
webpage = requests.get('https://www.ambitionbox.com/list-of-companies?page=1', headers=headers).text
```

### 4.2 Parsing with BeautifulSoup
```python
soup = BeautifulSoup(webpage, 'lxml')
```
This creates an object that allows us to search the HTML.

### 4.3 Extracting Data Points
We identify the container for each company and loop through them.
```python
company = soup.find_all('div', class_='company-content-wrapper')

for i in company:
    name.append(i.find('h2').text.strip())
    rating.append(i.find('p', class_='rating').text.strip())
    reviews.append(i.find('a', class_='review-count').text.strip())
    # ... extracting type, hq, old, employees
```

### 4.4 Handling Multiple Pages
Just like with APIs, we loop through page numbers to scrape multiple pages of results.
```python
final = pd.DataFrame()
for j in range(1, 1001):
    # ... request page j ...
    # ... extract data ...
    final = final.append(df)
```

## 5. Summary
Web scraping turns the entire internet into your dataset. It requires patience and inspection skills (using browser DevTools), but it unlocks data that is otherwise inaccessible.
