# API to DataFrame

## 1. Introduction
Static files are great, but many real-world pipelines rely on fresh data from web services. **APIs (Application Programming Interfaces)** let you query structured data on demand. This chapter is a practical, end‑to‑end guide: make robust HTTP requests, paginate results, normalize nested JSON, and assemble a clean Pandas DataFrame you can save and reuse.

![Client ↔ REST API request/response with pagination, generated by Nano Banana Model](https://via.placeholder.com/800x400?text=API+Request+Response+Cycle+by+Nano+Banana+Model)

## 2. Concepts You’ll Use

### HTTP Basics
- **Methods**: `GET` (read), `POST` (create), `PUT/PATCH` (update), `DELETE` (remove).
- **Query params**: `?page=1&api_key=xyz` filter/shape responses.
- **Status codes**: `200 OK`, `401 Unauthorized`, `403 Forbidden`, `404 Not Found`, `429 Too Many Requests` (rate limited), `5xx` server errors.

### Pagination Styles
- **Page-based**: `page=1, 2, …` with a `total_pages` field.
- **Cursor-based**: server returns a `next` token you pass to the subsequent request.

### JSON → DataFrame Normalization
API payloads contain nested objects/lists. Use `pandas.json_normalize` to flatten, select fields, and ensure consistent dtypes.

### Reliability Concerns
- **Rate limits**: Respect `429`; add backoff (`time.sleep` / exponential delay).
- **Auth**: API key in header (`Authorization: Bearer <token>`) or query param.
- **Retries**: Handle transient `5xx` with limited retries.
- **Idempotency**: Safe re‑runs shouldn’t duplicate rows.

## 3. Visual Walkthrough
Flow: Initialize → Request first page → Parse JSON → Select fields → Append → Check next page → Loop → Concatenate → Save CSV/Parquet.

## 4. Notebook Walkthrough (`day17.ipynb`)

### 4.1 First Request and Schema Probe
Inspect the shape of results before designing your table.
```python
import requests, pandas as pd
url = 'https://api.themoviedb.org/3/movie/top_rated'
params = {'api_key': 'YOUR_KEY', 'page': 1}
r = requests.get(url, params=params)
data = r.json()
pd.DataFrame(data['results']).head()
```

### 4.2 Selecting Fields and Normalizing
Pick a minimal, meaningful schema to avoid bloating your dataset.
```python
cols = ['id','title','overview','release_date','popularity','vote_average','vote_count']
temp = pd.DataFrame(data['results'])[cols]
```

### 4.3 Robust Pagination Loop
Handle pages, rate limit, and transient errors cleanly.
```python
import time
df_list = []
for page in range(1, data.get('total_pages', 1) + 1):
    resp = requests.get(url, params={'api_key':'YOUR_KEY','page':page})
    if resp.status_code == 429:
        time.sleep(2); continue
    resp.raise_for_status()
    results = resp.json().get('results', [])
    if not results: break
    df_list.append(pd.DataFrame(results)[cols])

df = pd.concat(df_list, ignore_index=True)
```

### 4.4 Data Quality Pass
- Convert `release_date` to datetime and sort.
- De‑duplicate on `id`.
- Validate ranges (`vote_average` in 0–10).
```python
df['release_date'] = pd.to_datetime(df['release_date'], errors='coerce')
df = df.drop_duplicates(subset=['id']).sort_values('popularity', ascending=False)
```

### 4.5 Persist
Choose CSV for portability or Parquet for performance.
```python
df.to_csv('movies.csv', index=False)
# or
df.to_parquet('movies.parquet', index=False)
```

## 5. Extensions and Best Practices
- Use a **Session** with retry/backoff (e.g., `requests.adapters.HTTPAdapter`).
- Parameterize secrets via environment variables, not hard‑coded keys.
- Cache pages locally to avoid re‑hitting rate limits during development.
- Document the API version and schema to keep pipelines reproducible.

## 6. Summary
APIs unlock living datasets. With solid request handling, pagination, normalization, and a quick quality pass, you can transform raw JSON streams into clean, analytics‑ready tables—on demand.
