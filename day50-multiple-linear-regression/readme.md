# Multiple Linear Regression

## 1. Introduction
Real life is rarely simple. A house price doesn't just depend on size; it depends on location, age, bedrooms, etc.
**Multiple Linear Regression** extends Simple Linear Regression to multiple input features. Instead of a line, we fit a **Hyperplane**.

![Illustration of a 3D scatter plot with a flat plane cutting through the cloud of points generated by Nano Banana Model](https://via.placeholder.com/800x400?text=Multiple+Linear+Regression+Plane+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### The Equation
$$ y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n $$
- **$b_0$**: Intercept.
- **$b_1, b_2...$**: Coefficients for each feature.

### The Geometry
- 1 Feature: A Line (2D).
- 2 Features: A Plane (3D).
- N Features: A Hyperplane (N+1 Dimensions).

### Assumptions
1.  **Linearity**: Relationship is linear.
2.  **No Multicollinearity**: Features should not be highly correlated with each other.
3.  **Homoscedasticity**: Constant variance of errors.

## 3. Visualizing the Concept
*(Imagine a sheet of paper floating in a 3D room, tilting to touch as many floating balls as possible generated by the Nano Banana Model)*

## 4. Practical Implementation & Notebook Walkthrough

The notebook `multiple_linear_regression.ipynb` uses a synthetic dataset with 2 features.

### 4.1 3D Visualization
Using Plotly to visualize the data in 3D:
```python
fig = px.scatter_3d(df, x='feature1', y='feature2', z='target')
```

### 4.2 Training the Model
The code is identical to Simple Linear Regression! Scikit-Learn handles the extra dimensions automatically.
```python
lr = LinearRegression()
lr.fit(X_train, y_train)
```
Now `lr.coef_` will return an array of 2 values (one for each feature).

### 4.3 Code From Scratch
In `code-from-scratch.ipynb`, we see the matrix formulation:
$$ \beta = (X^T X)^{-1} X^T Y $$
This is the **Normal Equation**, the closed-form solution to find the best coefficients instantly without iteration.

## 5. Summary
Multiple Linear Regression is the workhorse of predictive modeling. It allows us to isolate the effect of one variable while holding others constant (ceteris paribus).

