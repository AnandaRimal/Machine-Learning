# Logistic Regression (The Perceptron Trick)
# Logistic Regression (The Perceptron Trick)

## 1. Introduction
Welcome to **Classification**!
Despite the name, Logistic Regression is used for classification (predicting Yes/No, 0/1).
Before we get to the full algorithm, we need to understand its ancestor: **The Perceptron**. This chapter explores how we can iteratively find a line that separates two classes of data.

![Illustration of a line separating red dots from blue dots in a 2D plane generated by Nano Banana Model](https://via.placeholder.com/800x400?text=Linear+Classifier+Boundary+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### The Problem
We have two classes of data (e.g., Passed/Failed). We want to draw a line ($Ax + By + C = 0$) that separates them.

### The Perceptron Trick
1.  Start with a random line.
2.  Pick a random point.
3.  If the point is correctly classified, do nothing.
4.  If the point is **misclassified**, move the line slightly towards the point to fix the error.
5.  Repeat until most points are correct.

This simple rule is the foundation of Neural Networks!

## 3. Visualizing the Concept
*(Imagine a line jumping around a scatter plot. Every time it hits a red dot on the blue side, it nudges itself to correct the mistake generated by the Nano Banana Model)*

## 4. Practical Implementation & Notebook Walkthrough

The notebook `perceptron-trick.ipynb` implements this algorithm from scratch.

### 4.1 The Algorithm
```python
def perceptron(X, y):
    weights = np.ones(X.shape[1])
    lr = 0.1
    
    for i in range(1000):
        j = np.random.randint(0, 100) # Pick random point
        y_hat = step(np.dot(X[j], weights)) # Predict
        
        # Update weights if prediction is wrong
        weights = weights + lr * (y[j] - y_hat) * X[j]
```

### 4.2 The Step Function
The Perceptron uses a "Step Function" (output 0 or 1).
$$ 	ext{step}(z) = 1 	ext{ if } z > 0 	ext{ else } 0 $$
This is harsh. In the next chapter, we'll replace this with the **Sigmoid Function** to create true Logistic Regression.

### 4.3 Towards Logistic Regression
Replace the step with the smooth **Sigmoid** and optimize with **Gradient Descent** on the **Log-Loss**.
$$ \sigma(z) = \frac{1}{1+e^{-z}} \quad ; \quad \hat{y} = \sigma(w^T x + b) $$
Log-Loss:
$$ J = -\frac{1}{n}\sum \big(y\log(\hat{y}) + (1-y)\log(1-\hat{y})\big) $$
This yields probabilities, enabling thresholds, ROC curves, and calibration.

## 5. Summary
The Perceptron is a linear classifier. It works great if data is linearly separable. If not, it loops forever (unless we stop it). Logistic Regression improves on this by giving us *probabilities* instead of hard 0/1 predictions.
