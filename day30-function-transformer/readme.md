# Function Transformer

## 1. Introduction
Sometimes, standard scaling (Standardization/Normalization) isn't enough. If your data is heavily skewed (like Income or Population), it doesn't follow a Normal distribution (Bell curve). **FunctionTransformer** allows you to apply custom mathematical functions (Log, Reciprocal, Square Root) to transform your data into a Gaussian-like shape, which improves the performance of linear models.

![Comparison of skewed distribution vs log-transformed normal distribution generated by Nano Banana Model](https://via.placeholder.com/800x400?text=Log+Transformation+Effect+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### The Normality Assumption
Many algorithms (Linear Regression, Logistic Regression, LDA, Gaussian Naive Bayes) assume that the input features are Normally distributed. If they are right-skewed (long tail to the right), the model performs poorly.

### Mathematical Transformations
- **Log Transform (`np.log1p`)**: Best for right-skewed data. It compresses large values.
- **Reciprocal (`1/x`)**: Stronger than log.
- **Square Root (`np.sqrt`)**: Weaker than log.
- **Box-Cox / Yeo-Johnson**: Power transforms that automatically find the best exponent to make data normal.

## 3. Visualizing the Concept
*(Imagine a QQ Plot here generated by the Nano Banana Model: Before transform (curved line) vs. After transform (straight line on the diagonal))*

## 4. Practical Implementation & Notebook Walkthrough

The notebook `day30.ipynb` uses the Titanic dataset (`Age` and `Fare`) to test this.

### 4.1 Checking Normality
We use **QQ Plots** (Quantile-Quantile Plots) to visually check normality.
```python
import scipy.stats as stats
stats.probplot(X_train['Fare'], dist="norm", plot=plt)
```
*Observation*: The `Fare` column is heavily right-skewed (most tickets are cheap, a few are super expensive).

### 4.2 Applying FunctionTransformer
We use `FunctionTransformer` from `sklearn.preprocessing`.
```python
from sklearn.preprocessing import FunctionTransformer

# np.log1p calculates log(1+x) to avoid log(0) errors
trf = FunctionTransformer(func=np.log1p)

X_train_transformed = trf.fit_transform(X_train)
```

### 4.3 Measuring Impact
The notebook compares the accuracy of Logistic Regression (a linear model) before and after transformation.
- **Before**: Accuracy is lower because the model struggles with the skewed `Fare`.
- **After**: Accuracy increases significantly because the transformed `Fare` looks more "Normal".
- **Decision Tree**: Accuracy remains roughly the same because Trees don't care about distribution.

## 5. Summary
Function Transformers are a surgical tool. Use them when your data violates the normality assumption of your model. A simple Log transform can sometimes give you a free boost in accuracy.
