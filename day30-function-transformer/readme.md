# Function Transformer

## 1. Introduction
Sometimes, standard scaling (Standardization/Normalization) isn't enough. If your data is heavily skewed (like Income or Population), it doesn't follow a Normal distribution (Bell curve). **FunctionTransformer** allows you to apply custom mathematical functions (Log, Reciprocal, Square Root) to transform your data into a Gaussian-like shape, which improves the performance of linear models.

![Comparison of skewed distribution vs log-transformed normal distribution generated by Nano Banana Model](https://via.placeholder.com/800x400?text=Log+Transformation+Effect+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### The Normality Assumption
Many algorithms (Linear Regression, Logistic Regression, LDA, Gaussian Naive Bayes) assume that the input features are Normally distributed. If they are right-skewed (long tail to the right), the model performs poorly.

### Mathematical Transformations
- **Log Transform (`np.log1p`)**: Best for right-skewed data. It compresses large values.
- **Reciprocal (`1/x`)**: Stronger than log.
- **Square Root (`np.sqrt`)**: Weaker than log.
- **Box-Cox / Yeo-Johnson**: Power transforms that automatically find the best exponent to make data normal.

**Math**
- Box-Cox: $x' = \frac{x^\lambda - 1}{\lambda}$ for $\lambda \ne 0$; $x' = \log x$ for $\lambda=0$ (requires $x>0$).
- Yeo-Johnson handles zeros/negatives; finds $\lambda$ via MLE to approximate normality.

## 3. Visualizing the Concept
*(Imagine a QQ Plot here generated by the Nano Banana Model: Before transform (curved line) vs. After transform (straight line on the diagonal))*

## 4. Practical Implementation & Notebook Walkthrough

The notebook `day30.ipynb` uses the Titanic dataset (`Age` and `Fare`) to test this.

### 4.1 Checking Normality
We use **QQ Plots** (Quantile-Quantile Plots) to visually check normality.
```python
import scipy.stats as stats
stats.probplot(X_train['Fare'], dist="norm", plot=plt)
```
*Observation*: The `Fare` column is heavily right-skewed (most tickets are cheap, a few are super expensive).

### 4.2 Applying FunctionTransformer
We use `FunctionTransformer` from `sklearn.preprocessing`.
```python
from sklearn.preprocessing import FunctionTransformer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

# np.log1p calculates log(1+x) to avoid log(0) errors
trf = FunctionTransformer(func=np.log1p)

X_train_transformed = trf.fit_transform(X_train)
pre = ColumnTransformer([
	('log_fare', FunctionTransformer(np.log1p), ['Fare'])
], remainder='passthrough')

pipe = Pipeline([
	('pre', pre),
	('model', LogisticRegression(max_iter=1000))
])
pipe.fit(X_train, y_train)
print(pipe.score(X_test, y_test))
```

### 4.3 Measuring Impact
The notebook compares the accuracy of Logistic Regression (a linear model) before and after transformation.
- **Before**: Accuracy is lower because the model struggles with the skewed `Fare`.
- **After**: Accuracy increases significantly because the transformed `Fare` looks more "Normal".
- **Decision Tree**: Accuracy remains roughly the same because Trees don't care about distribution.

## 5. Summary
Function Transformers are a surgical tool. Use them when skew or non-normality hurts linear models. Prefer integrating transformations in Pipelines; consider PowerTransformer (Yeo-Johnson) for automated, robust normalization.
