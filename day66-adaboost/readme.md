# AdaBoost (Boosting)

## 1. Introduction
Random Forest trains trees in parallel (independently). **Boosting** trains them sequentially.
**AdaBoost (Adaptive Boosting)** trains a weak model, finds the mistakes, and trains the next model specifically to fix those mistakes. It turns weak learners into a strong learner.

![Illustration of a sequence of models where each subsequent model focuses on the red (wrong) points of the previous one generated by Nano Banana Model](https://via.placeholder.com/800x400?text=AdaBoost+Sequential+Learning+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### The Algorithm
1.  Assign equal weight to all data points.
2.  Train a **Decision Stump** (a tree with depth 1).
3.  Calculate error. Increase the weight of misclassified points.
4.  Train the next stump on the *weighted* data (it tries harder to get the hard points right).
5.  Repeat.
6.  Final prediction is a weighted vote of all stumps.

Each stump gets a weight based on its error:
$$ \alpha_t = \frac{1}{2} \ln\Big(\frac{1-\epsilon_t}{\epsilon_t}\Big) $$
Row weights update multiplicatively, emphasizing misclassified points.

### Weak Learners
AdaBoost uses "Decision Stumps"—very simple models that are just slightly better than guessing.

### Bias–Variance and Robustness
- Great at reducing bias by focusing on hard cases.
- Sensitive to noisy labels and outliers (can overweight bad points). Consider limiting `n_estimators` and using learning rate.

## 3. Visualizing the Concept
*(Imagine a teacher grading a test. The student fails Question 5. The teacher gives a new test with 10 variations of Question 5. The student learns. Repeat generated by the Nano Banana Model)*

## 4. Practical Implementation & Notebook Walkthrough

The notebook `adaboost_demo.ipynb` implements this step-by-step.

### 4.1 Manual Implementation
The notebook manually updates row weights:
```python
# Increase weights of wrong predictions
df['weights'] = ...
```
And plots the decision boundary shifting to cover the misclassified points.

### 4.2 Scikit-Learn
```python
from sklearn.ensemble import AdaBoostClassifier

ada = AdaBoostClassifier(n_estimators=50)
ada.fit(X_train, y_train)
```

Tune `n_estimators`, `learning_rate`, and base estimator depth to balance bias/variance and noise sensitivity.

## 5. Summary
Boosting is often more accurate than Bagging (Random Forest) but is more prone to overfitting and harder to tune. It is the basis for Gradient Boosting and XGBoost.
