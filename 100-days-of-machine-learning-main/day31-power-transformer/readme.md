# Power Transformer - Data-Driven Normalization\n\n## General Idea\n\nPowerTransformer is a scikit-learn preprocessing tool that applies power transformations to make data more Gaussian-like (normally distributed). Unlike fixed transformations like log or square root, PowerTransformer automatically learns the optimal transformation parameter from the data using either the Box-Cox or Yeo-Johnson method. The goal is to stabilize variance and minimize skewness, which can improve the performance of many machine learning algorithms that assume normality.\n\n## Why Use PowerTransformer?\n\n1. **Automatic Transformation**: Learns optimal transformation from data\n2. **Normality**: Makes distributions more Gaussian\n3. **Skewness Reduction**: Reduces both left and right skewness\n4. **Variance Stabilization**: Makes variance more consistent\n5. **Algorithm Performance**: Improves linear models, LDA, Naive Bayes\n6. **Outlier Mitigation**: Reduces influence of extreme values\n7. **Flexible**: Works with positive and negative values (Yeo-Johnson)\n8. **Standardization**: Optionally standardizes to zero mean, unit variance\n\n## Role in Machine Learning\n\n### Algorithms That Benefit\n\n**1. Linear Models**:\n- Linear Regression\n- Ridge/Lasso Regression\n- Logistic Regression\n\n**Why**: Assume normally distributed residuals\n\n**2. Linear Discriminant Analysis (LDA)**:\n- Assumes Gaussian class distributions\n- Improved decision boundaries with normal data\n\n**3. Gaussian Naive Bayes**:\n- Explicitly assumes normal distribution\n- Better probability estimates\n\n**4. K-Means Clustering**:\n- Assumes spherical clusters\n- Benefits from normalized scales\n\n**Algorithms That DON'T Need It**:\n- Tree-based: Random Forest, XGBoost (invariant to monotonic transformations)\n- SVM with RBF kernel (less sensitive)\n- Neural networks (can learn transformations)\n\n### The Normality Assumption\n\nMany statistical tests and ML algorithms assume:\n\n$$X \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\n\n**When violated**:\n- Biased estimates\n- Poor confidence intervals\n- Reduced predictive performance\n\n**PowerTransformer helps**:\n$$X' = T_{\\lambda}(X) \\approx \\mathcal{N}(0, 1)$$\n\nWhere $\\lambda$ is learned from data\n\n## Two Methods: Box-Cox vs Yeo-Johnson\n\n### Box-Cox Transformation\n\n**Mathematical Formula**:\n$$x'(\\lambda) = \\begin{cases}\n\\frac{x^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0 \\\\\n\\log(x) & \\text{if } \\lambda = 0\n\\end{cases}$$\n\n**Requirements**: $x > 0$ (strictly positive data only)\n\n**Parameter**:\n- $\\lambda$: Power parameter, learned from data\n- Typical range: $-5$ to $+5$\n- Common values:\n  - $\\lambda = 1$: No transformation\n  - $\\lambda = 0$: Log transformation\n  - $\\lambda = 0.5$: Square root\n  - $\\lambda = -1$: Reciprocal\n\n**How $\\lambda$ is chosen**:\n- Maximize log-likelihood of normality\n- Minimize skewness\n- Variance stabilization\n\n**Inverse Transformation**:\n$$x = \\begin{cases}\n(\\lambda x' + 1)^{1/\\lambda} & \\text{if } \\lambda \\neq 0 \\\\\ne^{x'} & \\text{if } \\lambda = 0\n\\end{cases}$$\n\n### Yeo-Johnson Transformation\n\n**Mathematical Formula**:\n$$x'(\\lambda) = \\begin{cases}\n\\frac{(x+1)^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0, x \\geq 0 \\\\\n\\log(x+1) & \\text{if } \\lambda = 0, x \\geq 0 \\\\\n-\\frac{(-x+1)^{2-\\lambda} - 1}{2-\\lambda} & \\text{if } \\lambda \\neq 2, x < 0 \\\\\n-\\log(-x+1) & \\text{if } \\lambda = 2, x < 0\n\\end{cases}$$\n\n**Advantage**: Works with **positive, negative, and zero** values\n\n**Requirements**: None (any real-valued data)\n\n**Use when**: Data contains zeros or negative values\n\n**Parameter**: $\\lambda$ learned same way as Box-Cox\n\n### Comparison\n\n| Aspect | Box-Cox | Yeo-Johnson |\n|--------|---------|-------------|\n| **Data Requirements** | $x > 0$ only | Any real values |\n| **Handles Zeros** | \u274c No | \u2713 Yes |\n| **Handles Negatives** | \u274c No | \u2713 Yes |\n| **Complexity** | Simpler formula | More complex |\n| **Speed** | Slightly faster | Slightly slower |\n| **Interpretation** | More intuitive | Less intuitive |\n| **Use Case** | Positive features (prices, counts) | Mixed-sign features (profits, temperatures) |\n\n**Default in sklearn**: Yeo-Johnson (more general)\n\n## PowerTransformer Syntax\n\n### Basic Usage\n\n```python\nfrom sklearn.preprocessing import PowerTransformer\n\n# Default: Yeo-Johnson\npt = PowerTransformer()\npt.fit(X_train)\nX_train_transformed = pt.transform(X_train)\nX_test_transformed = pt.transform(X_test)\n```\n\n### Parameters\n\n**method**: 'yeo-johnson' (default) or 'box-cox'\n- Transformation method to use\n\n**standardize**: bool, default=True\n- If True: Apply zero-mean, unit-variance scaling after transformation\n- If False: Only apply power transformation\n\n**copy**: bool, default=True\n- If True: Copy X, don't modify in-place\n\n**Full Syntax**:\n```python\npt = PowerTransformer(\n    method='yeo-johnson',  # or 'box-cox'\n    standardize=True,\n    copy=True\n)\n```\n\n### Fitted Attributes\n\n**lambdas_**: array of shape (n_features,)\n- Learned optimal $\\lambda$ for each feature\n- Access after fitting: `pt.lambdas_`\n\n**Example**:\n```python\npt.fit(X)\nprint(pt.lambdas_)  # [0.23, -0.45, 1.12, ...]\n```\n\n**Interpretation**:\n- $\\lambda \\approx 1$: Feature already near-normal, minimal transformation\n- $\\lambda \\approx 0$: Log-like transformation\n- $\\lambda < 0$: Reciprocal-like transformation\n- $\\lambda > 1$: Polynomial-like transformation\n\n## Transformation Process\n\n### Step-by-Step\n\n**1. For each feature $j$**:\n   - Find optimal $\\lambda_j$ that maximizes normality\n   \n**2. Apply transformation**:\n   $$x'_{ij} = T_{\\lambda_j}(x_{ij})$$\n   \n**3. If standardize=True**:\n   $$x''_{ij} = \\frac{x'_{ij} - \\mu'_j}{\\sigma'_j}$$\n\n**Result**: Each feature approximately $\\mathcal{N}(0, 1)$\n\n### Lambda Selection\n\n**Optimization Problem**:\n$$\\lambda^* = \\arg\\max_{\\lambda} \\mathcal{L}(X^{(\\lambda)})$$\n\nWhere $\\mathcal{L}$ is log-likelihood assuming normality:\n\n$$\\mathcal{L}(X^{(\\lambda)}) = -\\frac{n}{2}\\log(2\\pi) - \\frac{n}{2}\\log(\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i^{(\\lambda)} - \\mu)^2 + (\\lambda - 1)\\sum_{i=1}^{n}\\log(x_i)$$\n\n**Simplified objective**: Choose $\\lambda$ to minimize skewness and kurtosis\n\n**Skewness**:\n$$\\text{Skewness}(X^{(\\lambda)}) = \\frac{E[(X^{(\\lambda)} - \\mu)^3]}{\\sigma^3}$$\n\nTarget: $\\approx 0$ (symmetric)\n\n**Kurtosis**:\n$$\\text{Kurtosis}(X^{(\\lambda)}) = \\frac{E[(X^{(\\lambda)} - \\mu)^4]}{\\sigma^4}$$\n\nTarget: $\\approx 3$ (Gaussian kurtosis)\n\n## Mathematical Examples\n\n### Example 1: Right-Skewed Data (Box-Cox)\n\n**Original Data**: $X = [1, 2, 5, 10, 50, 100]$\n\n**Statistics**:\n- Mean: 28\n- Median: 7.5\n- Skewness: Highly positive (right-skewed)\n\n**PowerTransformer (Box-Cox)**:\n\nLearned: $\\lambda \\approx 0.15$ (near log transformation)\n\n**Transformation** (simplified):\n$$x' \\approx \\frac{x^{0.15} - 1}{0.15} \\approx \\text{log-like}$$\n\n**Transformed**: $X' \\approx [0, 1.5, 3.2, 4.5, 7.1, 8.5]$\n\n**New Statistics**:\n- Mean: 4.1\n- Median: 3.85\n- Skewness: $\\approx 0$ (symmetric)\n\n**If standardize=True**: Further scaled to mean=0, std=1\n\n### Example 2: Mixed Sign Data (Yeo-Johnson)\n\n**Original Data**: $X = [-10, -5, -1, 0, 2, 8, 20]$\n\n**Statistics**:\n- Skewness: Positive (due to large positive values)\n\n**PowerTransformer (Yeo-Johnson)**:\n\nLearned: $\\lambda \\approx 0.8$\n\n**Transformation**: Handles negatives and zeros automatically\n\n**Result**: More symmetric distribution\n\n### Example 3: Multiple Features\n\n**Data**:\n```\nFeature 1 (prices): [100, 200, 500, 1000] - right-skewed\nFeature 2 (counts): [1, 2, 3, 4]         - already normal\nFeature 3 (temps):  [-10, 0, 10, 20]    - symmetric\n```\n\n**Learned Lambdas**:\n- Feature 1: $\\lambda_1 \\approx 0.2$ (strong transformation)\n- Feature 2: $\\lambda_2 \\approx 1.0$ (minimal transformation)\n- Feature 3: $\\lambda_3 \\approx 0.9$ (slight transformation)\n\n**Interpretation**: PowerTransformer adapts to each feature's distribution\n\n## Using PowerTransformer in Pipelines\n\n### Basic Pipeline\n\n```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\n\npipeline = Pipeline([\n    ('power', PowerTransformer(method='yeo-johnson')),\n    ('model', LinearRegression())\n])\n\npipeline.fit(X_train, y_train)\npredictions = pipeline.predict(X_test)\n```\n\n**Flow**:\n$$X \\xrightarrow{\\text{PowerTransform}} X' \\xrightarrow{\\text{LinearReg}} \\hat{y}$$\n\n### With ColumnTransformer\n\n**Apply to specific columns**:\n```python\nfrom sklearn.compose import ColumnTransformer\n\npreprocessor = ColumnTransformer([\n    ('power', PowerTransformer(), ['price', 'income', 'age']),\n    ('onehot', OneHotEncoder(), ['category', 'region']),\n    ('passthrough', 'passthrough', ['binary_feature'])\n])\n\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression())\n])\n```\n\n**Benefit**: Only transform features that need normalization\n\n### Comparing Methods\n\n**Try both methods with GridSearchCV**:\n```python\nfrom sklearn.model_selection import GridSearchCV\n\npipeline = Pipeline([\n    ('power', PowerTransformer()),\n    ('model', Ridge())\n])\n\nparam_grid = {\n    'power__method': ['box-cox', 'yeo-johnson'],\n    'power__standardize': [True, False],\n    'model__alpha': [0.1, 1, 10]\n}\n\ngrid = GridSearchCV(pipeline, param_grid, cv=5)\ngrid.fit(X_train, y_train)\n\nprint(grid.best_params_)\n# Example: {'power__method': 'yeo-johnson', 'power__standardize': True, 'model__alpha': 1}\n```\n\n## Inverse Transform\n\n### Purpose\n\nConvert transformed data back to original scale\n\n**Use case**: Predictions in transformed space \u2192 original scale\n\n### Implementation\n\n```python\npt = PowerTransformer()\npt.fit(X_train)\n\n# Transform\nX_transformed = pt.transform(X_train)\n\n# Inverse transform\nX_original = pt.inverse_transform(X_transformed)\n# X_original \u2248 X_train (within numerical precision)\n```\n\n### Pipeline Predictions\n\n**Problem**: Model predicts in transformed space\n\n**Solution**: Inverse transform predictions\n\n```python\n# Pipeline with target transformation\nfrom sklearn.compose import TransformedTargetRegressor\n\nmodel = TransformedTargetRegressor(\n    regressor=LinearRegression(),\n    transformer=PowerTransformer()\n)\n\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)  # Automatically inverse-transformed!\n```\n\n**How it works**:\n1. Fit: Transform $y_{train}$, fit model on $y'_{train}$\n2. Predict: Model outputs $\\hat{y}'$, inverse transform to $\\hat{y}$\n\n## Comparison with Alternatives\n\n### vs StandardScaler\n\n**StandardScaler**:\n- Simple linear transformation: $z = \\frac{x - \\mu}{\\sigma}$\n- Doesn't change distribution shape\n- Fast\n\n**PowerTransformer**:\n- Non-linear transformation\n- Changes distribution shape\n- Makes more Gaussian\n- Slower\n\n**When to use StandardScaler**: Data already normal, just need scaling\n\n**When to use PowerTransformer**: Skewed data, need normality\n\n### vs QuantileTransformer\n\n**PowerTransformer**:\n- Parametric (assumes power law structure)\n- Learns single parameter $\\lambda$ per feature\n- Preserves some original relationships\n- Better for moderately skewed data\n\n**QuantileTransformer**:\n- Non-parametric (learns full quantile mapping)\n- More flexible\n- Can achieve perfect uniformity/normality\n- Better for heavily skewed or multi-modal data\n- More prone to overfitting\n\n**Mathematical difference**:\n\n**PowerTransformer**: $x' = T_{\\lambda}(x)$ (1 parameter)\n\n**QuantileTransformer**: $x' = F^{-1}(\\hat{F}(x))$ (full CDF)\n\n### vs Log Transform (FunctionTransformer)\n\n**FunctionTransformer(np.log1p)**:\n- Fixed transformation: $x' = \\log(1 + x)$\n- No parameter learning\n- Manual choice\n\n**PowerTransformer**:\n- Adaptive: Learns optimal $\\lambda$ (could be near log if $\\lambda \\approx 0$)\n- Data-driven\n- May choose different transformation\n\n**Example**:\n- If optimal $\\lambda = 0.1$, PowerTransformer similar to log\n- If optimal $\\lambda = 0.5$, PowerTransformer similar to sqrt\n- If optimal $\\lambda = 1.2$, PowerTransformer uses polynomial\n\n## When to Use PowerTransformer\n\n### Use When:\n\n1. **Skewed Features**: Distribution has long tail\n2. **Linear Models**: Using regression, LDA, Naive Bayes\n3. **Normality Needed**: Algorithm assumes Gaussian distribution\n4. **Variance Heterogeneity**: Variance changes with mean\n5. **Residual Issues**: Model residuals are non-normal\n6. **Automatic Selection**: Want data-driven transformation choice\n\n### Don't Use When:\n\n1. **Tree-Based Models**: Random Forest, XGBoost (invariant to monotonic transforms)\n2. **Already Normal**: Features already approximately Gaussian\n3. **Categorical After Encoding**: One-hot encoded variables (binary, shouldn't transform)\n4. **Small Sample Size**: Need $n > 50$ per feature for reliable $\\lambda$ estimation\n5. **Interpretability Critical**: Transformation makes coefficients harder to interpret\n\n## Best Practices\n\n### 1. Check Data Requirements\n\n**For Box-Cox**:\n```python\n# Ensure all values > 0\nif (X > 0).all():\n    pt = PowerTransformer(method='box-cox')\nelse:\n    pt = PowerTransformer(method='yeo-johnson')\n```\n\n### 2. Fit on Training Data Only\n\n**Correct**:\n```python\npt.fit(X_train)  # Learn lambdas from train\nX_train_t = pt.transform(X_train)\nX_test_t = pt.transform(X_test)  # Use train's lambdas\n```\n\n**Incorrect**:\n```python\npt.fit(X_all)  # Data leakage!\n```\n\n### 3. Visualize Before and After\n\n```python\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\n# Before\naxes[0].hist(X[:, 0], bins=30)\naxes[0].set_title('Before PowerTransform')\n\n# After\nX_transformed = pt.fit_transform(X)\naxes[1].hist(X_transformed[:, 0], bins=30)\naxes[1].set_title(f'After PowerTransform (λ={pt.lambdas_[0]:.2f})')\n```\n\n### 4. Check Lambda Values\n\n```python\npt.fit(X)\nfor i, lambda_val in enumerate(pt.lambdas_):\n    print(f\"Feature {i}: λ = {lambda_val:.3f}\")\n    \n    if abs(lambda_val - 1) < 0.1:\n        print(\"  → Minimal transformation (already normal)\")\n    elif abs(lambda_val) < 0.1:\n        print(\"  → Log-like transformation\")\n    elif abs(lambda_val - 0.5) < 0.1:\n        print(\"  → Square root-like transformation\")\n```\n\n### 5. Use standardize=True for Most Cases\n\n**Default**: `standardize=True`\n\n**Reason**: Most algorithms benefit from zero mean, unit variance\n\n**Exception**: If you want to apply your own scaling afterward\n\n### 6. Apply Only to Continuous Features\n\n**Don't transform**:\n- Binary variables (0/1)\n- One-hot encoded features\n- Already transformed features\n- Categorical numeric codes\n\n**Use ColumnTransformer**:\n```python\nColumnTransformer([\n    ('power', PowerTransformer(), continuous_features),\n    ('passthrough', 'passthrough', binary_features)\n])\n```\n\n### 7. Consider Sample Size\n\n**Minimum recommended**: $n \\geq 50$ observations\n\n**Reason**: Need sufficient data to reliably estimate $\\lambda$\n\n**Small samples**: Consider fixed transformations (log, sqrt) instead\n\n## Common Issues and Solutions\n\n### Issue 1: All Values Negative or Zero (Box-Cox)\n\n**Error**: `ValueError: Box-Cox can only be applied to strictly positive data`\n\n**Solution**:\n```python\n# Option 1: Use Yeo-Johnson\npt = PowerTransformer(method='yeo-johnson')\n\n# Option 2: Shift data to positive range (then Box-Cox)\nX_shifted = X - X.min() + 1\npt = PowerTransformer(method='box-cox')\npt.fit(X_shifted)\n```\n\n### Issue 2: Transformation Not Effective\n\n**Symptom**: Data still looks skewed after transformation\n\n**Causes**:\n1. Outliers dominating distribution\n2. Multi-modal distribution\n3. Inherently non-normal structure\n\n**Solutions**:\n```python\n# Try QuantileTransformer instead\nfrom sklearn.preprocessing import QuantileTransformer\nqt = QuantileTransformer(output_distribution='normal')\n\n# Or remove outliers first\nfrom sklearn.preprocessing import RobustScaler\npipeline = Pipeline([\n    ('outlier_clip', FunctionTransformer(clip_outliers)),\n    ('power', PowerTransformer())\n])\n```\n\n### Issue 3: Inverse Transform Doesn't Match Original\n\n**Symptom**: `inverse_transform(transform(X)) != X`\n\n**Cause**: Numerical precision, especially with extreme values\n\n**Check**:\n```python\nX_reconstructed = pt.inverse_transform(pt.transform(X))\nerror = np.abs(X - X_reconstructed).max()\nprint(f\"Max reconstruction error: {error}\")\n# Should be < 1e-10 for most data\n```\n\n### Issue 4: Lambda Values Unexpected\n\n**Symptom**: $\\lambda$ very large or very small (e.g., $|\\lambda| > 5$)\n\n**Causes**:\n1. Outliers influencing estimation\n2. Insufficient data\n3. Distribution truly unusual\n\n**Investigation**:\n```python\nprint(f\"Lambdas: {pt.lambdas_}\")\n\n# Check distribution\nimport scipy.stats as stats\nfor i in range(X.shape[1]):\n    skew = stats.skew(X[:, i])\n    print(f\"Feature {i}: skewness = {skew:.2f}, λ = {pt.lambdas_[i]:.2f}\")\n```\n\n## Summary\n\nPowerTransformer is a powerful preprocessing tool that automatically learns optimal transformations to make data more Gaussian, improving performance for algorithms that assume normality.\n\n**Key Concepts**:\n\n**Core Functionality**:\n- Learns optimal power parameter $\\lambda$ per feature\n- Reduces skewness, stabilizes variance\n- Makes distributions approximately normal\n\n**Two Methods**:\n\n**Box-Cox**: $x'(\\lambda) = \\frac{x^\\lambda - 1}{\\lambda}$ (for $x > 0$)\n\n**Yeo-Johnson**: Extended version (for any $x \\in \\mathbb{R}$)\n\n**Transformation Process**:\n1. Find optimal $\\lambda$ (maximize normality likelihood)\n2. Apply power transformation\n3. Optionally standardize (zero mean, unit variance)\n\n**When to Use**:\n- Skewed features\n- Linear models (regression, LDA)\n- Algorithms assuming normality\n- Automatic transformation selection\n\n**When NOT to Use**:\n- Tree-based models\n- Data already normal\n- Binary/categorical features\n- Small sample sizes ($n < 50$)\n\n**Best Practices**:\n- Use Yeo-Johnson for mixed-sign data\n- Fit on training data only\n- Set standardize=True (usually)\n- Visualize before/after\n- Check lambda values for insights\n- Apply only to continuous features\n\n**Comparison**:\n- **vs StandardScaler**: PowerTransformer changes shape, not just scale\n- **vs QuantileTransformer**: PowerTransformer parametric, less flexible\n- **vs Log Transform**: PowerTransformer data-driven, adaptive\n\n**Advantages**:\n- Automatic optimal transformation\n- Improves model performance\n- Handles both positive/negative (Yeo-Johnson)\n- Invertible (can get original scale back)\n\n**Limitations**:\n- Requires sufficient data\n- May not help tree-based models\n- Can reduce interpretability\n- Doesn't guarantee perfect normality\n\nPowerTransformer is an essential tool for preprocessing pipelines when working with algorithms that benefit from normally distributed features, providing data-driven transformation selection without manual tuning.\n\n---\n\n**Video Link**: https://youtu.be/lV_Z4HbNAx0"
