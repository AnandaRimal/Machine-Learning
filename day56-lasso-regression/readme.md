# Lasso Regression (L1 Regularization)

## 1. Introduction
Ridge shrinks coefficients, but it keeps them all. What if you want to get rid of useless features entirely?
**Lasso (Least Absolute Shrinkage and Selection Operator)** is a regularization technique that can perform **Feature Selection**. It shrinks coefficients of unimportant features to exactly **Zero**.

![Illustration of a lasso rope tightening around coefficients and eliminating the weak ones generated by Nano Banana Model](https://via.placeholder.com/800x400?text=Lasso+Feature+Selection+by+Nano+Banana+Model)

## 2. Conceptual Deep Dive

### The Cost Function
$$ J = \sum (y - \hat{y})^2 + \lambda \sum |\beta| $$
- **$\sum |\beta|$**: The sum of absolute values of coefficients (L1 Norm).

### The "Diamond" Geometry
Why does Lasso zero out coefficients?
Geometrically, the L1 constraint creates a diamond shape. The error contours often hit the "corners" of the diamond first, where one of the coefficients is exactly zero. Ridge creates a circle, where corners don't exist.

## 3. Visualizing the Concept
*(Imagine a slider controlling Alpha. As you slide it up, feature weights drop to zero one by one, leaving only the most important features generated by the Nano Banana Model)*

## 4. Practical Implementation & Notebook Walkthrough

The notebook `lasso-regression-demo.ipynb` visualizes this effect.

### 4.1 Lasso in Scikit-Learn
```python
from sklearn.linear_model import Lasso

# alpha controls the strength
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)
```

### 4.2 Feature Selection in Action
The notebook plots the regression line for different alphas.
- **Alpha=0**: Same as Linear Regression.
- **Alpha=High**: The line becomes flat (slope=0), meaning the feature was removed.

## 5. Summary
Use **Lasso** when you have a lot of features and you suspect only a few are important (Sparse solution). Use **Ridge** when you think most features are useful. 
